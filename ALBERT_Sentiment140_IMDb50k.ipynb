{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+a6Q3wMF9lRj2ue5aZuy+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ALBERT Sentiment Analysis Experiments on Sentiment 140 Dataset using baseline model (and with PEFT -- LoRA, BitFit, Prompt Tuning)"],"metadata":{"id":"CYZdoclQKV2I"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veUZygUX1tRj","outputId":"661832b4-52e2-42d1-be1d-fbe3338ba0e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["\"\"\"We begin our process by installing packages such as pytorch, which is used extensively here, as well as HuggingFace's\n","transformers and datasets packages, which are used to run the ALBERT transformer model and load the Sentiment140 dataset, respectively. \"\"\"\n","\n","!pip install torch transformers datasets -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WaxE4q9J5EV2"},"outputs":[],"source":["\"\"\"This step configures the credentials of the active user to seemlessly enable push and pull to and from the group's X-PERTS github repository\"\"\"\n","\n","!git config --global credential.helper store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfmG3I-T6LSn"},"outputs":[],"source":["\"\"\"We next import the installed packages, namely the ALBERT model\"\"\"\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n","\n","import time\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"970697f8-c698-4a1e-b0dc-efa8e85d7818"},"outputs":[],"source":["\"\"\" We next instantiate (load) our temporary dataset, calling to our sentiment140.py script with a simple one-liner where this script handles the rest\"\"\"\n","\n","dataset = load_dataset(\"./sentiment140.py\", name=\"sentiment140\")\n","full_train = dataset[\"train\"]\n","\n","print(\"Train size:\", len(dataset[\"train\"]))\n","print(\"Test size:\", len(dataset[\"test\"]))"]},{"cell_type":"code","source":["\"\"\" We next import a few packages for randomization of our sampling, re for text recognition and denoising and Datasets for creating our downsampled dataset\"\"\"\n","\n","import random\n","import re\n","from datasets import Dataset"],"metadata":{"id":"UiwQ92_Xad_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" With the entire 1.6m entry dataset loaded in as full_train above, we next filter out entries labelled 2 (a 'neutral' sentiment class label,\n","though there seemed not to be any such instances), and we define negative and positive tweets, from which we randomly sample 25k instances each for class balance.\n","We lastly overwrite our dataset with just the 50k class-balanced records from pre-processing to ensure fast, tractable training is possible run locally given\n","our resource-efficiency computationally-constrained focus.\"\"\"\n","\n","all_data = [x for x in full_train if x[\"sentiment\"] in [0,4]]\n","\n","negative = [x for x in all_data if x[\"sentiment\"] == 0]\n","positive = [x for x in all_data if x[\"sentiment\"] == 4]\n","\n","random.seed(42)\n","\n","negative_sample = random.sample(negative, 25000)\n","positive_sample = random.sample(positive, 25000)\n","\n","sampled_data = negative_sample + positive_sample\n","\n","random.shuffle(sampled_data)\n","\n","dataset = Dataset.from_list(sampled_data)"],"metadata":{"id":"4cRuBD7Eaevl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Next, with our 50k sentiment dataset, we perform pre-processing for standardization and to remove\n","noise in the form of mentions (e.g. @gatech), URLs (e.g. https://...), hashtags (e.g. #SwineFlu),\n","non-alphanumeric characters (e.g. emojis, capitalization, punctuation)\"\"\"\n","\n","def clean_text(text):\n","  text = text.lower()\n","  text = re.sub(r\"http\\S+\", \"\", text)\n","  text = re.sub(r\"@\\w+\", \"\", text)\n","  text = re.sub(r\"#\\w+\", \"\", text)\n","  text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n","  return text.strip()\n","\n","dataset = dataset.map(lambda x: {\"text\": clean_text(x[\"text\"])})"],"metadata":{"id":"foNSxSaqalri"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300,"referenced_widgets":["6237ee14eb76420ea4f0b00ec2c429d7","eb3c9c5779ef40089ef03fa31be391af","bdb937fc686c4b6a8718acf1ffe79f34","32dc049651384edd861700c3688222a7","d4a01c1d1d6b4abcbe1cffc15b6f77da","0f5a317c92bd44b3a50239cc2003add9","1306c9c2070c49859516ed6e81664a45","14daacb74e764e7fa8837aad37fa6e85","044a502e62c5468c9378207a07088079","1c29c1b029d941138f6cf5a6b309975d","88eec9ad20744565b44899c42325e242","98fee8644dc34b63b0e971f4e15b73dc","88eb510e1fa94e6bb7ac10b82cfdb10c","741f23b7a1264b9f90a98825808b2e14","afe3260013174b2faa3bee6d85a5038b","c8857d36ff41427494a62f3fc9fd7851","12c8235eacce471c8d87108e2d282cd9","14589d46fa1142aead6095b6ef977f4a","12e04fedeac342a3aa1868484e20be50","64ea6e844cea42e8a8c4acd2c66d357c","60bace568fdb4ee885da2af5ef0a2811","248723bb73274730b42acf377e5db0bc","73b2b3e9a0194ffc91fa17174a3a56a5","fd9e55de3c6c4f90995ae54256da74e3","134d37b253ba4cdbbd65504eafacffea","11bacb8a9f4e414fb02e5ce36618ed85","9257e5bc2f2b4cdab4a34ff4bf93e816","3da388a19ce34bc5a5193251c59d9f77","f57e909c63fa4c71b82a2762a22257ab","0ac56303727a4d05a57ca2554002f357","68f1af78221e458e88a8a6d89e80c87b","710b81cbb4124262bafcd634d4fcc776","67f080ba67874917886e72dbc6a23d9a","654af0f2611b495d9fedc6fb31a6e42c","3d5727aa91ed42b280dbb38a901436f6","d73b57405b644957bf3cf2110f5578f9","5176926467ae4e8aa2cc746bea95d7df","a1fbd83f0f224f51ad725de3bf72c292","e92f5baefc474071b2f0c57d8047de4d","67151232362849708a24cc3f0872efa5","28f3005caa944c7c870427b3a502788c","ebdd79e2a91b4db6a7aff938a5f427e1","cc29f085002a4ccd963c78db05192fca","006cd7e16d554cf19df74d46e6f9af38","db115df5eb734d9586c6e65b0e249848","fe165e0a411b45bc815dc8cb8bf94152","a835e17fb3454feaa3e4df3c7aa2329b","bfb4910f78924b63bccf8abc7cb3f2c0","466a32afac2847b3b516748b9e7a4173","bc7f63c9e8f4441a85f697a646979bb1","27e847b1304748a2bdd0577eb354f4d0","7cdb15e7c3e94928bb9d9ec2bef1e096","99754e54baa0460486fde7827e071487","28f564aee43e4c9f97b9ac2acf434353","4868e5022b2b48e2bb9bab29bb829b45","19ebc8bbb912450bb0e3a830dc0e7a8a","f7c8c52e73bd4eabb760c4beb2fe821e","eaa821e6b39942159cb3860b830968e5","8c39643929b246cabdd6bb8e4d60d1f7","3d2edb44040c4e37bf8dca05b5d1c484","c9bacbb9bba441069588d872341046ec","43300c29859f4bde8c08b3ed1ef3523f","3e6c6355f41f4a85af51dccf6e198c39","c2223ecfc5d74523a36a20ed3b1f3d54","f590c23799204d80a2599b2e7b059ba5","a741e086a94f489b9b190f7e494bf644"]},"id":"Ui9EkziIuH1a","outputId":"fd825a32-5eb9-4940-cde2-0bc80296b6a7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6237ee14eb76420ea4f0b00ec2c429d7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98fee8644dc34b63b0e971f4e15b73dc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73b2b3e9a0194ffc91fa17174a3a56a5","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"654af0f2611b495d9fedc6fb31a6e42c","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db115df5eb734d9586c6e65b0e249848","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19ebc8bbb912450bb0e3a830dc0e7a8a","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_name = \"albert-base-v2\"\n","num_labels = 2\n","tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, token=token)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["89327a82a44a4dffb4dfab9f03bc8a02","0594b04063be4958ba93a90931d97144","ef5ea9971fb143df8ab8368fb474a8a5","0bca6ff35e7e4ecda9784200e1e2f7c3","f0914e98184545f59e96474673e3a55a","ad7d76774e1b40c8bb5b7a67449dbe8e","5a72ec0bc3e9481faac1d3f54694ed7f","292884d87e8f46308c3bc81a1be418c7","2a4a9b3f41ea4e3aae0fad370a0dae4d","cced608a485d4ecd824dfa6a35620d65","bde684c2936544aeb505d28aa3dd14e9","c64aafc505c942aebe2efe0761cda932","93353622d7a04c53adaaadccdfbd6fe0","3dc638060da14f4793d49ac3e5dd7a1d","9dc5ae1ca29d4137a7e36a9bb966ad58","a825822648de4fd2bdd4393889908422","1151605950304e14a7faf435ed3bc8db","e73e009e623e4952a3d47b6a375ab8a9","a6e66f0b8dfc4550a140231545875cb3","bdaebf2aa33c4d62a5f9a9e7380fe253","44b03334c76d465aa54a0f4d7045a473","7efeca5eb90744188b359562e5341b5c","c1ccc88e46164aa1abab04e293fbf0c8","d7412fd30a5a4c42b68de099c9295d5b","111585bede3e49fc9070c8d9d98682a5","ce43400950884671b0e390985450005f","0f13b9a495324274beba18050f53083f","fa84e7abb86340a796d152f3ab94e71f","6a41f9df8baf48cd94d0fbe30b1afff5","b22ce667e40f4197bfe5605763ffafb0","fb8850e5ff6a41fe821d783e9de28ce1","a0a8b5d7a6bd44f5b3e86541d0556832","938177b1981d48599c5e451ce1ec17d6"]},"id":"x3LDE7G78kZi","outputId":"0a9c2cf8-1f83-47f0-81db-1721630905bc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89327a82a44a4dffb4dfab9f03bc8a02","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1600000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c64aafc505c942aebe2efe0761cda932","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1ccc88e46164aa1abab04e293fbf0c8","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/498 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\" We apply mapping from the seemingly arbitrary 0,4 scale to 0,1 for standard binary classification\"\"\"\n","\n","def map_labels(example):\n","    example[\"sentiment\"] = 0 if example[\"sentiment\"] == 0 else 1\n","    return example\n","\n","dataset = dataset.map(map_labels)\n","\n","print(\"Dataset size:\", len(dataset))\n","\n","from collections import Counter\n","print(\"Class distribution:\", Counter(dataset[\"sentiment\"]))"]},{"cell_type":"markdown","metadata":{"id":"SZg_vTpeckxT"},"source":["DOWNSAMPLING FOR TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238,"referenced_widgets":["e9b9092262a04a75a572398d785ffb7e","c30bcd950bf044bcad2d8d7d8499e366","197fd04cc8c84ab98b83194d74cd7ce5","a3cc7e4e173c4a569461b2db0f129a59","2c9e878ed7de461b8fd11aaebe8904cb","ff7e19a0c21a4a3e8f17629e411611d3","65466690e1234bd5b0fa1f9cb9b09d52","c6ba7be25219480baef5ede9f1983f73","c86088d675e94dcdab53d421f298dfde","ff3be868c4ee4a5d99d22e29a566a063","48a1fa929402454688b953a77000eb94","e8ad21434f3940fda0ef480fab4e7057","b5ee8af6bb2b4a41b33d4f6e241a6123","3149d83125f14a25a68f07d9bb7ec527","5fbce850a881496797329d6090bba788","b42c82a596d845edbe6910892006ad8f","4be008f659094b7aadc8f8ead33312ad","a1f4e3fdd14b421d8fc31db0af2c7a83","e84bde4db2334fe299227f4b53b3cafd","12ece26a67004f7b8b424137a5ce9679","90eff6665f5d4ac4a09c28fd8241efa8","9db8a09a9dbb45c0ba8f5a409d37e14b","e69ee956d0e04024ba354070bb648819","3dfcdbe7e374417292b666e1b75668b0","413a4cbc6b294729bd04fec658a48096","624cd547c6034d8b9448dc3545c35bf7","02cfd1d435c04d8196de39d3c824cb69","3cbd67bccbe64d18997241ccf57b3085","2860fdcb1a744ef594fd2cd1397e38cf","1d50685f71954b89a402b6e379be2875","94cdfa83f8564ea48174857be8d56a18","4d5acf4de2a14745b32b9752977b21ed","2755295e16f74cd4b26b60210e17cc45"]},"id":"Lce-qJ-R2EjF","outputId":"007a2ffe-9cea-4987-9732-53c00712dc41"},"outputs":[{"name":"stdout","output_type":"stream","text":["160000\n","112000\n","24000\n","24000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9b9092262a04a75a572398d785ffb7e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/112000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8ad21434f3940fda0ef480fab4e7057","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e69ee956d0e04024ba354070bb648819","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Train:     112000\n","Validation:24000\n","Test:      24000\n"]}],"source":["\"\"\" We next perform downsampling and stratified splitting of the data 80/20 Train/Test \"\"\"\n","\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","from collections import Counter\n","import pandas as pd\n","\n","df = dataset.to_pandas()\n","df = df.dropna(subset=[\"text\", \"sentiment\"])\n","\n","print(\"Initial class distribution:\", Counter(df[\"sentiment\"]))\n","\n","df_train, df_val_test = train_test_split(\n","    df,\n","    stratify=df[\"sentiment\"],\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","print(\"Train size:\", len(df_train))\n","print(\"Val size:\", len(df_val))\n","print(\"Test size:\", len(df_test))\n","print(\"Train class distribution:\", Counter(df_train[\"sentiment\"]))\n","print(\"Test class distribution:\", Counter(df_test[\"sentiment\"]))\n","\n","train_dataset = Dataset.from_pandas(df_train).remove_columns([\"__index_level_0__\"])\n","test_dataset  = Dataset.from_pandas(df_test).remove_columns([\"__index_level_0__\"])\n","\n","def tokenize(example):\n","    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","train_dataset = train_dataset.map(tokenize, batched=True)\n","test_dataset  = test_dataset.map(tokenize, batched=True)\n","\n","train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"sentiment\"])\n","test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"sentiment\"])\n","\n","tokenized_dataset = DatasetDict({\n","    \"train\": train_dataset,\n","    \"test\": test_dataset\n","})"]},{"cell_type":"code","source":["\"\"\" We print the head of each of the train/test sets to visualize our cleaned data\"\"\"\n","\n","print(\"\\nSample training examples:\")\n","display(df_train.head(5))\n","\n","print(\"\\nSample test examples:\")\n","display(df_test.head(5))"],"metadata":{"id":"S3vHiQypa5Nw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d5728f3-570f-464b-99ac-ceab061235ef"},"outputs":[],"source":["\"\"\" We initialize our dataloader for each of the sets, fix their batch sizes\n","and randomize their order\"\"\"\n","\n","train_loader = DataLoader(tokenized_dataset[\"train\"], batch_size=16, shuffle=True)\n","test_loader  = DataLoader(tokenized_dataset[\"test\"], batch_size=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6f7b3205-82c7-43e7-8346-c14157263461","outputId":"1b7cfad2-9616-46ec-d9c3-bfbe65e8b7ee"},"outputs":[{"data":{"text/plain":["OPTForSequenceClassification(\n","  (model): OPTModel(\n","    (decoder): OPTDecoder(\n","      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n","      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n","      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x OPTDecoderLayer(\n","          (self_attn): OPTSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): ReLU()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (score): Linear(in_features=768, out_features=2, bias=False)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" We define our optimizer using Adam and set a conservative learning rate and\n","weight decay (though later hyperparameter search will overwrite)\"\"\"\n","\n","optimizer = torch.optim.AdamW(\n","    filter(lambda p: p.requires_grad, model.parameters()),\n","    lr=5e-5,\n","    weight_decay=0.01\n",")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"id":"uH4BBUaCvezF","outputId":"890c90ab-8b1b-4a35-e78d-795dd7e85e18"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test Accuracy   : 0.4466\n","F1 Score (macro): 0.4008\n","F1 Score (weighted): 0.4008\n","Inference Time  : 139.21s\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.38      0.17      0.24     12000\n","    Positive       0.47      0.72      0.57     12000\n","\n","    accuracy                           0.45     24000\n","   macro avg       0.42      0.45      0.40     24000\n","weighted avg       0.42      0.45      0.40     24000\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 600x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\" Baseline inference for binary sentiment analysis task run on ALBERT\n","without PEFT (i.e. without BitFit and/or LoRA)\"\"\"\n","\n","import time\n","import torch\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","inference_start = time.time()\n","\n","model.eval()\n","total_correct = 0\n","total_samples = 0\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"sentiment\"].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","\n","        all_preds.extend(predictions.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","        total_correct += (predictions == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","accuracy = total_correct / total_samples\n","f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n","f1_weighted = f1_score(all_labels, all_preds, average=\"weighted\")\n","inference_time = time.time() - inference_start\n","\n","\n","print(f'\\nBaseline Inference Performance - ALBERT on Sentiment140\\n')\n","print(f\"\\nTest Accuracy   : {accuracy:.4f}\")\n","print(f\"F1 Score (macro): {f1_macro:.4f}\")\n","print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","print(f\"Inference Time  : {inference_time:.2f}s\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"]))\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - Baseline Inference (ALBERT on Sentiment140)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vLHL9VzpvrR0"},"source":["# LORA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ffwlr2XOvq1s","outputId":"93e8b109-915e-4ee4-bfb3-e5320c7b10c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n","Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"]}],"source":["\"\"\" Install Parameter Efficient Finetuning Packages (e.g. LoRA and BitFit)\"\"\"\n","\n","!pip install peft -q"]},{"cell_type":"code","source":["\"\"\" Importing LoRA packages \"\"\"\n","\n","import gc\n","import torch\n","import time\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n","from peft import get_peft_model, LoraConfig, TaskType\n","from sklearn.metrics import classification_report, f1_score\n","from torch.utils.data import DataLoader"],"metadata":{"id":"poexjA7inJRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" LoRA parameter setup \"\"\"\n","\n","learning_rates = [5e-5, 1e-4]\n","batch_sizes = [8, 16]\n","epochs = 6"],"metadata":{"id":"pjz6krAJnGzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Training on ALBERT model using LoRA and output dataset generation (saved as .csv)\"\"\"\n","\n","results = []\n","\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","        print(f\"Running LoRA with LR={lr}, batch_size={batch_size}\")\n","\n","        # loading ALBERT model\n","        model_name = \"albert-base-v2\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","        # LoRA param update config\n","        lora_config = LoraConfig(\n","            task_type=TaskType.SEQ_CLS,\n","            r=16,\n","            lora_alpha=32,\n","            lora_dropout=0.1,\n","            bias=\"none\",\n","            target_modules=[\"query\", \"key\", \"value\"]\n","        )\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","\n","        # instantiate dataloader\n","        data_collator = DataCollatorWithPadding(tokenizer)\n","        train_dataloader = DataLoader(full_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n","        test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n","\n","        # adam optimizer\n","        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n","\n","        # begin training\n","        model.train()\n","        start_time = time.time()\n","        epoch_logs = []\n","\n","        for epoch in range(1, epochs + 1):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            loop = tqdm(train_dataloader, leave=False)\n","            for step, batch in enumerate(loop):\n","                if step >= 300:\n","                    break\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                correct += (preds == batch['labels']).sum().item()\n","                total += batch['labels'].size(0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            avg_train_loss = running_loss / (step + 1)\n","            train_accuracy = correct / total\n","\n","            # perform per epoch evaluation\n","            model.eval()\n","            val_running_loss = 0.0\n","            y_true, y_pred = [], []\n","            inference_start = time.time()\n","            with torch.no_grad():\n","                for batch in test_dataloader:\n","                    batch = {k: v.to(device) for k, v in batch.items()}\n","                    outputs = model(**batch)\n","                    preds = torch.argmax(outputs.logits, dim=1)\n","                    y_true.extend(batch[\"labels\"].cpu().numpy())\n","                    y_pred.extend(preds.cpu().numpy())\n","                    val_running_loss += outputs.loss.item()\n","\n","            avg_val_loss = val_running_loss / len(test_dataloader)\n","            inference_time = time.time() - inference_start\n","\n","            report = classification_report(y_true, y_pred, output_dict=True)\n","            val_accuracy = report[\"accuracy\"]\n","            val_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","            epoch_logs.append({\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": avg_train_loss,\n","                \"train_accuracy\": train_accuracy,\n","                \"val_loss\": avg_val_loss,\n","                \"val_accuracy\": val_accuracy\n","            })\n","\n","            if epoch == epochs:\n","                total_correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n","                total_samples = len(y_true)\n","                accuracy = total_correct / total_samples\n","                f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n","                f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n","\n","                print(f\"\\n[Final Epoch {epoch}] Inference Metrics:\")\n","                print(f\"Test Accuracy      : {accuracy:.4f}\")\n","                print(f\"F1 Score (macro)   : {f1_macro:.4f}\")\n","                print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","                print(f\"Inference Time     : {inference_time:.2f} seconds\")\n","                print(\"\\nClassification Report: ALBERT w/ LoRA on Sentiment140\\n\")\n","                print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n","\n","            model.train()\n","\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # begin datalogging per lr/bs\n","        epoch_logs_df = pd.DataFrame(epoch_logs)\n","        epoch_logs_df.to_csv(f\"sent_albert_lora_epoch_logs_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # saver inference metrics per lr/bs\n","        metrics_summary_df = pd.DataFrame(report).transpose()\n","        metrics_summary_df.to_csv(f\"sent_albert_lora_inference_metrics_summary_lr{lr}_bs{batch_size}.csv\", index=True)\n","\n","        # save inference predictions for the final epoch\n","        predictions_df = pd.DataFrame({\n","            \"y_true\": y_true,\n","            \"y_pred\": y_pred\n","        })\n","        predictions_df.to_csv(f\"sent_albert_lora_inference_predictions_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # log memory usage\n","        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n","\n","        # save model params and metrics\n","        results.append({\n","            \"method\": \"LoRA\",\n","            \"learning_rate\": lr,\n","            \"batch_size\": batch_size,\n","            \"accuracy\": val_accuracy,\n","            \"f1\": val_f1,\n","            \"training_time\": training_time,\n","            \"inference_time\": inference_time,\n","            \"max_memory\": max_memory\n","        })\n","\n","        # empty cache to conserve compute\n","        del model, tokenizer, optimizer\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","# ranked performance by val acc\n","results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n","\n","# save overall results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"sent_albert_lora_results.csv\", index=False)\n","\n","# save best final config and metrics\n","final_summary_df = pd.DataFrame({\n","    \"Method\": [\"LoRA\"],\n","    \"Best LR\": [results[0][\"learning_rate\"]],\n","    \"Best Batch Size\": [results[0][\"batch_size\"]],\n","    \"Accuracy\": [results[0][\"accuracy\"]],\n","    \"F1 Score\": [results[0][\"f1\"]],\n","    \"Training Time (s)\": [results[0][\"training_time\"]],\n","    \"Inference Time (s)\": [results[0][\"inference_time\"]],\n","    \"Max GPU Memory (GB)\": [results[0][\"max_memory\"]]\n","})\n","final_summary_df.to_csv(\"sent_albert_lora_final_comparison_lora.csv\", index=False)\n","\n","print(\"All LoRA Grid Search Results:\")\n","for r in results:\n","    print(r)\n","\n","print(\"\\nBest LoRA Configuration:\")\n","print(results[0])"],"metadata":{"id":"xMZxdkoQjsVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_best_lr = results[0][\"learning_rate\"]\n","lora_best_bs = results[0][\"batch_size\"]\n","\n","# Construct filename\n","best_report_file = f\"sent_albert_lora_inference_metrics_summary_lr{lora_best_lr}_bs{lora_best_bs}.csv\"\n","\n","# Load the saved best report\n","best_report_df = pd.read_csv(best_report_file)\n","print(\"\\nClassification Report for Best Configuration:\")\n","print(best_report_df)\n","\n","\n","best_preds_df = pd.read_csv(f\"sent_albert_lora_inference_predictions_lr{lora_best_lr}_bs{lora_best_bs}.csv\")\n","print(\"\\nInference Predictions for Best Configuration:\")\n","print(best_preds_df)\n","\n","y_true = best_preds_df[\"y_true\"]\n","y_pred = best_preds_df[\"y_pred\"]\n","\n","\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - ALBERT w/ LoRA on Sentiment140 \\n\")\n","plt.show()"],"metadata":{"id":"vc5K_8MLcdNv","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"error","timestamp":1745732643337,"user_tz":240,"elapsed":175,"user":{"displayName":"Jack Henderson","userId":"11950955252015448237"}},"outputId":"6449a0bb-6a70-441f-df7e-d283533db5a6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'results' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-49c4272d92e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Construct filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_report_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"opt_lora_inference_metrics_summary_lr{best_lr}_bs{best_bs}.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"2CDtkIN7_gfP"},"source":["# BITFIT"]},{"cell_type":"code","source":["\"\"\" Importing BitFit packages \"\"\"\n","\n","import gc\n","import torch\n","import time\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n","from peft import get_peft_model, LoraConfig, TaskType\n","from sklearn.metrics import classification_report, f1_score\n","from torch.utils.data import DataLoader"],"metadata":{"id":"Xez7WEn3gVSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" BitFit parameter setup \"\"\"\n","learning_rates = [5e-5, 1e-4]\n","batch_sizes = [8, 16]\n","epochs = 6"],"metadata":{"id":"ux6y-fXGgd4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Training on ALBERT model using BitFit and output dataset generation (saved as .csv)\"\"\"\n","\n","results = []\n","\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","        print(f\"Running BitFit with LR={lr}, batch_size={batch_size}\")\n","\n","        # loading ALBERT model\n","        model_name = \"albert-base-v2\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","        # BitFit param update config\n","        for name, param in model.named_parameters():\n","            if \"bias\" in name:\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","\n","        # instantiate dataloader\n","        data_collator = DataCollatorWithPadding(tokenizer)\n","        train_dataloader = DataLoader(full_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n","        test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n","\n","        # adam optimizer\n","        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n","\n","        # begin training\n","        model.train()\n","        start_time = time.time()\n","        epoch_logs = []\n","\n","        for epoch in range(1, epochs + 1):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            loop = tqdm(train_dataloader, leave=False)\n","            for step, batch in enumerate(loop):\n","                if step >= 300:\n","                    break\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                correct += (preds == batch['labels']).sum().item()\n","                total += batch['labels'].size(0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            avg_train_loss = running_loss / (step + 1)\n","            train_accuracy = correct / total\n","\n","            # perform per epoch evaluation\n","            model.eval()\n","            val_running_loss = 0.0\n","            y_true, y_pred = [], []\n","            inference_start = time.time()\n","            with torch.no_grad():\n","                for batch in test_dataloader:\n","                    batch = {k: v.to(device) for k, v in batch.items()}\n","                    outputs = model(**batch)\n","                    preds = torch.argmax(outputs.logits, dim=1)\n","                    y_true.extend(batch[\"labels\"].cpu().numpy())\n","                    y_pred.extend(preds.cpu().numpy())\n","                    val_running_loss += outputs.loss.item()\n","\n","            avg_val_loss = val_running_loss / len(test_dataloader)\n","\n","            inference_time = time.time() - inference_start\n","\n","            report = classification_report(y_true, y_pred, output_dict=True)\n","            val_accuracy = report[\"accuracy\"]\n","            val_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","            epoch_logs.append({\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": avg_train_loss,\n","                \"train_accuracy\": train_accuracy,\n","                \"val_loss\": avg_val_loss,\n","                \"val_accuracy\": val_accuracy\n","            })\n","\n","            if epoch == epochs:\n","                total_correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n","                total_samples = len(y_true)\n","                accuracy = total_correct / total_samples\n","                f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n","                f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n","\n","                print(f\"\\n[Final Epoch {epoch}] Inference Metrics:\")\n","                print(f\"Test Accuracy      : {accuracy:.4f}\")\n","                print(f\"F1 Score (macro)   : {f1_macro:.4f}\")\n","                print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","                print(f\"Inference Time     : {inference_time:.2f} seconds\")\n","                print(\"\\nClassification Report: ALBERT w/ BitFit on Sentiment140\\n\")\n","                print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n","\n","            model.train()\n","\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # begin datalogging per lr/bs\n","        epoch_logs_df = pd.DataFrame(epoch_logs)\n","        epoch_logs_df.to_csv(f\"sent_albert_bitfit_epoch_logs_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # saver inference metrics per lr/bs\n","        metrics_summary_df = pd.DataFrame(report).transpose()\n","        metrics_summary_df.to_csv(f\"sent_albert_bitfit_inference_metrics_summary_lr{lr}_bs{batch_size}.csv\", index=True)\n","\n","        # save inference predictions for the final epoch\n","        predictions_df = pd.DataFrame({\n","            \"y_true\": y_true,\n","            \"y_pred\": y_pred\n","        })\n","        predictions_df.to_csv(f\"sent_albert_bitfit_inference_predictions_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # log memory usage\n","        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n","\n","        # save model params and metrics\n","        results.append({\n","            \"method\": \"BitFit\",\n","            \"learning_rate\": lr,\n","            \"batch_size\": batch_size,\n","            \"accuracy\": val_accuracy,\n","            \"f1\": val_f1,\n","            \"training_time\": training_time,\n","            \"inference_time\": inference_time,\n","            \"max_memory\": max_memory\n","        })\n","\n","        # empty cache to conserve compute\n","        del model, tokenizer, optimizer\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","# ranked performance by val acc\n","results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n","\n","# save overall results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"sent_albert_bitfit_results.csv\", index=False)\n","\n","# save best final config and metrics\n","final_summary_df = pd.DataFrame({\n","    \"Method\": [\"BitFit\"],\n","    \"Best LR\": [results[0][\"learning_rate\"]],\n","    \"Best Batch Size\": [results[0][\"batch_size\"]],\n","    \"Accuracy\": [results[0][\"accuracy\"]],\n","    \"F1 Score\": [results[0][\"f1\"]],\n","    \"Training Time (s)\": [results[0][\"training_time\"]],\n","    \"Inference Time (s)\": [results[0][\"inference_time\"]],\n","    \"Max GPU Memory (GB)\": [results[0][\"max_memory\"]]\n","})\n","final_summary_df.to_csv(\"sent_albert_bf_final_comparison_bitfit.csv\", index=False)\n","\n","print(\"All BitFit Grid Search Results:\")\n","for r in results:\n","    print(r)\n","\n","print(\"\\nBest BitFit Configuration:\")\n","print(results[0])\n"],"metadata":{"id":"Ue0a3mDldMgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bf_best_lr = results[0][\"learning_rate\"]\n","bf_best_bs = results[0][\"batch_size\"]\n","\n","# Construct filename\n","best_report_file = f\"sent_albert_bitfit_inference_metrics_summary_lr{bf_best_lr}_bs{bf_best_bs}.csv\"\n","\n","# Load the saved best report\n","best_report_df = pd.read_csv(best_report_file)\n","print(\"\\nClassification Report for Best Configuration:\")\n","print(best_report_df)\n","\n","\n","best_preds_df = pd.read_csv(f\"sent_albert_bitfit_inference_predictions_lr{bf_best_lr}_bs{bf_best_bs}.csv\")\n","print(\"\\nInference Predictions for Best Configuration:\")\n","print(best_preds_df)\n","\n","y_true = best_preds_df[\"y_true\"]\n","y_pred = best_preds_df[\"y_pred\"]\n","\n","\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - ALBERT w/ BitFit on Sentiment140\")\n","plt.show()"],"metadata":{"id":"ZiIBDhw_oDq6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prompt Tuning"],"metadata":{"id":"chbf8LBOn6Rr"}},{"cell_type":"code","source":["\"\"\" Importing prompt tuning packages from PEFT \"\"\"\n","\n","import gc\n","from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType"],"metadata":{"id":"vPTqLRyhn-Ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Prompt tuning parameter setup \"\"\"\n","\n","lrs = [5e-5, 1e-4]\n","bs = [8, 16]\n","num_tokens = 20\n","epochs = 6"],"metadata":{"id":"JXozUOxQoSkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Training and evaluation loop with hyperparamter grid search \"\"\"\n","\n","results = []\n","\n","for lr in lrs:\n","    for batch_size in bs:\n","        print(f\"Running Prompt Tuning with LR={lr}, batch_size={batch_size}\")\n","\n","        # loading ALBERT model\n","        model_name = \"albert-base-v2\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","        # prompt tuning config\n","        peft_config = PromptTuningConfig(\n","            task_type=TaskType.SEQ_CLS,\n","            num_virtual_tokens=num_tokens,\n","            tokenizer_name_or_path=tokenizer.name_or_path,\n","            prompt_tuning_init=PromptTuningInit.RANDOM,\n","        )\n","        prompt_model = get_peft_model(model, peft_config)\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        prompt_model.to(device)\n","\n","        # instantiate dataloader\n","        data_collator = DataCollatorWithPadding(tokenizer)\n","        train_dataloader = DataLoader(full_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n","        test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n","\n","        # adam optimization\n","        optimizer = torch.optim.AdamW(prompt_model.parameters(), lr=lr)\n","\n","        # begin training\n","        prompt_model.train()\n","        start_time = time.time()\n","        epoch_logs = []\n","\n","        for epoch in range(1, epochs + 1):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            loop = tqdm(train_dataloader, leave=False)\n","            for step, batch in enumerate(loop):\n","                if step >= 300:\n","                    break\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = prompt_model(**batch)\n","                loss = outputs.loss\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                correct += (preds == batch['labels']).sum().item()\n","                total += batch['labels'].size(0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            avg_train_loss = running_loss / (step + 1)\n","            train_accuracy = correct / total\n","\n","            # perform per epoch evaluation\n","            prompt_model.eval()\n","            val_running_loss = 0.0\n","            y_true, y_pred = [], []\n","            with torch.no_grad():\n","                for batch in test_dataloader:\n","                    batch = {k: v.to(device) for k, v in batch.items()}\n","                    outputs = prompt_model(**batch)\n","                    preds = torch.argmax(outputs.logits, dim=1)\n","                    y_true.extend(batch[\"labels\"].cpu().numpy())\n","                    y_pred.extend(preds.cpu().numpy())\n","                    val_running_loss += outputs.loss.item()\n","\n","            avg_val_loss = val_running_loss / len(test_dataloader)\n","\n","            inference_time = time.time() - start_time\n","\n","            report = classification_report(y_true, y_pred, output_dict=True)\n","            val_accuracy = report[\"accuracy\"]\n","            val_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","            # print classification report on final epoch\n","            if epoch == epochs:\n","                total_correct = np.sum(np.array(y_true) == np.array(y_pred))\n","                total_samples = len(y_true)\n","\n","                accuracy = total_correct / total_samples\n","                f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n","                f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n","\n","                print(f\"\\n[Final Epoch {epoch}] Inference Metrics:\")\n","                print(f\"Test Accuracy      : {accuracy:.4f}\")\n","                print(f\"F1 Score (macro)   : {f1_macro:.4f}\")\n","                print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","                print(f\"Inference Time     : {inference_time:.2f} seconds\")\n","                print(\"\\nClassification Report: - ALBERT w/ Prompt Tuning on Sentiment140\\n\")\n","                print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n","\n","            epoch_logs.append({\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": avg_train_loss,\n","                \"train_accuracy\": train_accuracy,\n","                \"val_loss\": avg_val_loss,\n","                \"val_accuracy\": val_accuracy\n","            })\n","\n","            prompt_model.train()\n","\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # begin datalogging per lr/bs\n","        epoch_logs_df = pd.DataFrame(epoch_logs)\n","        epoch_logs_df.to_csv(f\"sent_albert_prompt_epoch_logs_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # save inference metrics per lr/bs\n","        metrics_summary_df = pd.DataFrame(report).transpose()\n","        metrics_summary_df.to_csv(f\"sent_albert_prompt_inference_metrics_summary_lr{lr}_bs{batch_size}.csv\", index=True)\n","\n","        # Save inference predictions for the final epoch\n","        predictions_df = pd.DataFrame({\n","            \"y_true\": y_true,\n","            \"y_pred\": y_pred\n","        })\n","        predictions_df.to_csv(f\"sent_albert_prompt_inference_predictions_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # log memory usage\n","        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n","\n","        # save model params and metrics\n","        results.append({\n","            \"method\": \"Prompt Tuning\",\n","            \"learning_rate\": lr,\n","            \"batch_size\": batch_size,\n","            \"accuracy\": val_accuracy,\n","            \"f1\": val_f1,\n","            \"training_time\": training_time,\n","            \"max_memory\": max_memory\n","        })\n","\n","        # empty cache to conserve compute\n","        del prompt_model, model, tokenizer, optimizer\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","# ranked performance by val acc\n","results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n","\n","# save overall results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"sent_albert_prompt_results.csv\", index=False)\n","\n","# save best final config and metrics\n","final_summary_df = pd.DataFrame({\n","    \"Method\": [\"Prompt Tuning\"],\n","    \"Best LR\": [results[0][\"learning_rate\"]],\n","    \"Best Batch Size\": [results[0][\"batch_size\"]],\n","    \"Accuracy\": [results[0][\"accuracy\"]],\n","    \"F1 Score\": [results[0][\"f1\"]],\n","    \"Training Time (s)\": [results[0][\"training_time\"]],\n","    \"Max GPU Memory (GB)\": [results[0][\"max_memory\"]]\n","})\n","final_summary_df.to_csv(\"sent_albert_prompt_final_comparison_prompt_tuning.csv\", index=False)\n","\n","print(\"All Prompt Tuning Grid Search Results:\")\n","for r in results:\n","    print(r)\n","\n","print(\"\\nBest Configuration:\")\n","print(results[0])"],"metadata":{"id":"QGndptLI-Y6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_best_lr = results[0][\"learning_rate\"]\n","prompt_best_bs = results[0][\"batch_size\"]\n","\n","# Construct filename\n","best_report_file = f\"sent_albert_prompt_inference_metrics_summary_lr{prompt_best_lr}_bs{prompt_best_bs}.csv\"\n","\n","# Load the saved best report\n","best_report_df = pd.read_csv(best_report_file)\n","print(\"\\nClassification Report for Best Configuration:\")\n","print(best_report_df)\n","\n","\n","best_preds_df = pd.read_csv(f\"sent_albert_prompt_inference_predictions_lr{prompt_best_lr}_bs{prompt_best_bs}.csv\")\n","print(\"\\nInference Predictions for Best Configuration:\")\n","print(best_preds_df)\n","\n","y_true = best_preds_df[\"y_true\"]\n","y_pred = best_preds_df[\"y_pred\"]\n","\n","\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - ALBERT w/ Prompt Tuning on Sentiment140\\n\")\n","plt.show()"],"metadata":{"id":"mbiSsXhaoe6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Begin Visualization of Sentiment140 Results"],"metadata":{"id":"MrnofdyEfB3G"}},{"cell_type":"markdown","source":["Load all dataframes from above training of 3 PEFT methods"],"metadata":{"id":"1tyd9uc4483i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohrZSoX4O7rF"},"outputs":[],"source":["\"\"\" Output from our ALBERT_Sentiment140.ipynb file, we read in the .csv files here.\n","Note that these .csv file paths suggest they should be uploaded to session storage\"\"\"\n","\n","# ALBERT PEFT-wise results across bf/lora/prompt tuning\n","albert_bf_results = pd.read_csv('/sent_albert_bitfit_results.csv')\n","albert_lora_results = pd.read_csv('/sent_albert_lora_results.csv')\n","albert_prompt_results = pd.read_csv('/sent_albert_prompt_results.csv')\n","\n","# ALBERT per-epoch performance logs (for LC generation) across bf/lora/prompt tuning\n","albert_lora_epochs_lr5_bs8 = pd.read_csv('/sent_albert_lora_epoch_logs_lr5e-05_bs8.csv')\n","albert_lora_epochs_lr5_bs16 = pd.read_csv('/sent_albert_lora_epoch_logs_lr5e-05_bs16.csv')\n","albert_lora_epochs_lr1_bs8 = pd.read_csv('/sent_albert_lora_epoch_logs_lr1e-04_bs8.csv')\n","albert_lora_epochs_lr1_bs16 = pd.read_csv('/sent_albert_lora_epoch_logs_lr1e-04_bs16.csv')\n","albert_bf_epochs_lr5_bs8 = pd.read_csv('/sent_albert_bitfit_epoch_logs_lr5e-05_bs8.csv')\n","albert_bf_epochs_lr5_bs16 = pd.read_csv('/sent_albert_bitfit_epoch_logs_lr5e-05_bs16.csv')\n","albert_bf_epochs_lr1_bs8 = pd.read_csv('/sent_albert_bitfit_epoch_logs_lr1e-04_bs8.csv')\n","albert_bf_epochs_lr1_bs16 = pd.read_csv('/sent_albert_bitfit_epoch_logs_lr1e-04_bs16.csv')\n","albert_prompt_epochs_lr5_bs8 = pd.read_csv('/sent_albert_prompt_epoch_logs_lr5e-05_bs8.csv')\n","albert_prompt_epochs_lr5_bs16 = pd.read_csv('/sent_albert_prompt_epoch_logs_lr5e-05_bs16.csv')\n","albert_prompt_epochs_lr1_bs8 = pd.read_csv('/sent_albert_prompt_epoch_logs_lr1e-04_bs8.csv')\n","albert_prompt_epochs_lr1_bs16 = pd.read_csv('/sent_albert_prompt_epoch_logs_lr1e-04_bs16.csv')\n","\n","# ALBERT inference performance metric summary across bf/lora/prompt tuning\n","albert_bf_inf_lr5_bs8 = pd.read_csv('/sent_albert_bitfit_inference_metrics_summary_lr5e-05_bs8.csv')\n","albert_bf_inf_lr5_bs16 = pd.read_csv('/sent_albert_bitfit_inference_metrics_summary_lr5e-05_bs16.csv')\n","albert_bf_inf_lr1_bs8 = pd.read_csv('/sent_albert_bitfit_inference_metrics_summary_lr1e-04_bs8.csv')\n","albert_bf_inf_lr1_bs16 = pd.read_csv('/sent_albert_bitfit_inference_metrics_summary_lr1e-04_bs16.csv')\n","albert_lora_inf_lr5_bs8 = pd.read_csv('/sent_albert_lora_inference_metrics_summary_lr5e-05_bs8.csv')\n","albert_lora_inf_lr5_bs16 = pd.read_csv('/sent_albert_lora_inference_metrics_summary_lr5e-05_bs16.csv')\n","albert_lora_inf_lr1_bs8 = pd.read_csv('/sent_albert_lora_inference_metrics_summary_lr1e-04_bs8.csv')\n","albert_lora_inf_lr1_bs16 = pd.read_csv('/sent_albert_lora_inference_metrics_summary_lr1e-04_bs16.csv')\n","albert_prompt_inf_lr5_bs8 = pd.read_csv('/sent_albert_prompt_inference_metrics_summary_lr5e-05_bs8.csv')\n","albert_prompt_inf_lr5_bs16 = pd.read_csv('/sent_albert_prompt_inference_metrics_summary_lr5e-05_bs16.csv')\n","albert_prompt_inf_lr1_bs8 = pd.read_csv('/sent_albert_prompt_inference_metrics_summary_lr1e-04_bs8.csv')\n","albert_prompt_inf_lr1_bs16 = pd.read_csv('/sent_albert_prompt_inference_metrics_summary_lr1e-04_bs16.csv')\n","\n","# ALBERT inference predictions across bf/lora/prompt tuning\n","albert_bf_preds_lr5_bs8 = pd.read_csv('/sent_albert_bitfit_inference_predictions_lr5e-05_bs8.csv')\n","albert_bf_preds_lr5_bs16 = pd.read_csv('/sent_albert_bitfit_inference_predictions_lr5e-05_bs16.csv')\n","albert_bf_preds_lr1_bs8 = pd.read_csv('/sent_albert_bitfit_inference_predictions_lr1e-04_bs8.csv')\n","albert_bf_preds_lr1_bs16 = pd.read_csv('/sent_albert_bitfit_inference_predictions_lr1e-04_bs16.csv')\n","albert_lora_preds_lr5_bs8 = pd.read_csv('/sent_albert_lora_inference_predictions_lr5e-05_bs8.csv')\n","albert_lora_preds_lr5_bs16 = pd.read_csv('/sent_albert_lora_inference_predictions_lr5e-05_bs16.csv')\n","albert_lora_preds_lr1_bs8 = pd.read_csv('/sent_albert_lora_inference_predictions_lr1e-04_bs8.csv')\n","albert_lora_preds_lr1_bs16 = pd.read_csv('/sent_albert_lora_inference_predictions_lr1e-04_bs16.csv')\n","albert_prompt_preds_lr5_bs8 = pd.read_csv('/sent_albert_prompt_inference_predictions_lr5e-05_bs8.csv')\n","albert_prompt_preds_lr5_bs16 = pd.read_csv('/sent_albert_prompt_inference_predictions_lr5e-05_bs16.csv')\n","albert_prompt_preds_lr1_bs8 = pd.read_csv('/sent_albert_prompt_inference_predictions_lr1e-04_bs8.csv')\n","albert_prompt_preds_lr1_bs16 = pd.read_csv('/sent_albert_prompt_inference_predictions_lr1e-04_bs16.csv')\n","\n","# ALBERT PEFT method intra-comparison based on hyperparameter settings, per bf/lora/prompt tuning\n","albert_bf_final_comparison = pd.read_csv('/sent_albert_bf_final_comparison_bitfit.csv')\n","albert_lora_final_comparison = pd.read_csv('/sent_albert_lora_final_comparison_lora.csv')\n","albert_prompt_final_comparison = pd.read_csv('/sent_albert_prompt_final_comparison_prompt_tuning.csv')"]},{"cell_type":"markdown","source":["BitFit Learning Curves"],"metadata":{"id":"UhiBBlSi47Ju"}},{"cell_type":"code","source":["# All BitFit Train/Val Acc Learning Curve\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs16\")\n","plt.title(\"Learning Curve: Accuracy - ALBERT w/ BitFit on Sentiment140\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# All BitFit Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs16\")\n","plt.title(\"Learning Curve: Loss - ALBERT w/ BitFit on Sentiment140\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"XPl-IFncvfzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best BitFit Train/Val Acc Learning Curve\n","\n","albert_bf_epochs_map = {\n","    (5, 8): albert_bf_epochs_lr5_bs8,\n","    (5, 16): albert_bf_epochs_lr5_bs16,\n","    (1, 8): albert_bf_epochs_lr1_bs8,\n","    (1, 16): albert_bf_epochs_lr1_bs16\n","}\n","\n","bf_lr_mapping = {\n","    5e-5: 5,\n","    1e-4: 1\n","}\n","\n","bf_best_lr_tag = bf_lr_mapping[bf_best_lr]\n","bf_best_bs_tag = bf_best_bs\n","\n","bf_epochs = albert_bf_epochs_map[(bf_best_lr_tag, bf_best_bs_tag)]\n","\n","# Best BitFit Training and Validation Accuracy\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"train_accuracy\", label=\"Training Accuracy\")\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"val_accuracy\", label=\"Validation Accuracy\")\n","plt.title(f\"Learning Curve: Best Accuracy - ALBERT w/ BitFit on Sentiment140\\nBest Hyperparameters: LR: {bf_best_lr} and BS: {bf_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Best BitFit Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"train_loss\", label=\"Training Loss\")\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n","plt.title(f\"Learning Curve: Best Loss - ALBERT w/ BitFit on Sentiment140\\nBest Hyperparameters: LR: {bf_best_lr} and BS: {bf_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"du5tvNL7vfOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LoRA Learning Curves"],"metadata":{"id":"csLwqvuu411j"}},{"cell_type":"code","source":["# All LoRA Train/Val Acc Learning Curve\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs16\")\n","plt.title(\"Learning Curve: Accuracy - ALBERT w/ LoRA on Sentiment140\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# All LoRA Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs16\")\n","plt.title(\"Learning Curve: Loss - ALBERT w/ LoRA on Sentiment140\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"sjyFwlew4oW3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best LoRA Train/Val Acc Learning Curve\n","\n","albert_lora_epochs_map = {\n","    (5, 8): albert_lora_epochs_lr5_bs8,\n","    (5, 16): albert_lora_epochs_lr5_bs16,\n","    (1, 8): albert_lora_epochs_lr1_bs8,\n","    (1, 16): albert_lora_epochs_lr1_bs16\n","}\n","\n","lora_lr_mapping = {\n","    5e-5: 5,\n","    1e-4: 1\n","}\n","\n","lora_best_lr_tag = lora_lr_mapping[lora_best_lr]\n","lora_best_bs_tag = lora_best_bs\n","\n","lora_epochs = albert_lora_epochs_map[(lora_best_lr_tag, lora_best_bs_tag)]\n","\n","# Best LoRA Training and Validation Accuracy\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"train_accuracy\", label=\"Training Accuracy\")\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"val_accuracy\", label=\"Validation Accuracy\")\n","plt.title(f\"Learning Curve: Best Accuracy - ALBERT w/ LoRA on Sentiment140\\nBest Hyperparameters: LR: {lora_best_lr} and BS: {lora_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Best LoRA Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"train_loss\", label=\"Training Loss\")\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n","plt.title(f\"Learning Curve: Best Loss - ALBERT w/ LoRA on Sentiment140\\nBest Hyperparameters: LR: {lora_best_lr} and BS: {lora_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"YPbFPEzC30L-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prompt Tuning Learning Curves"],"metadata":{"id":"tz5cCJ4O5FRd"}},{"cell_type":"code","source":["# All Prompt Tuning Train/Val Acc Learning Curve\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs16\")\n","plt.title(\"Learning Curve: Accuracy - ALBERT w/ Prompt Tuning on Sentiment140\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# All Prompt Tuning Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs16\")\n","plt.title(\"Learning Curve: Loss - ALBERT w/ Prompt Tuning on Sentiment140\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"19i4gc705HTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best Prompt Tuning Train/Val Acc Learning Curve\n","\n","albert_prompt_epochs_map = {\n","    (5, 8): albert_prompt_epochs_lr5_bs8,\n","    (5, 16): albert_prompt_epochs_lr5_bs16,\n","    (1, 8): albert_prompt_epochs_lr1_bs8,\n","    (1, 16): albert_prompt_epochs_lr1_bs16\n","}\n","\n","prompt_lr_mapping = {\n","    5e-5: 5,\n","    1e-4: 1\n","}\n","\n","prompt_best_lr_tag = prompt_lr_mapping[prompt_best_lr]\n","prompt_best_bs_tag = prompt_best_bs\n","\n","prompt_epochs = albert_prompt_epochs_map[(prompt_best_lr_tag, prompt_best_bs_tag)]\n","\n","# Best Prompt Tuning Training and Validation Accuracy\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"train_accuracy\", label=\"Training Accuracy\")\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"val_accuracy\", label=\"Validation Accuracy\")\n","plt.title(f\"Learning Curve: Best Accuracy - ALBERT w/ Prompt Tuning on Sentiment140\\nBest Hyperparameters: LR: {prompt_best_lr} and BS: {prompt_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Best Prompt Tuning Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"train_loss\", label=\"Training Loss\")\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n","plt.title(f\"Learning Curve: Best Loss - ALBERT w/ Prompt Tuning on Sentiment140\\nBest Hyperparameters: LR: {prompt_best_lr} and BS: {prompt_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"7GGrrOE35Wpf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PEFT Method Comparison:"],"metadata":{"id":"lgxYLthl6Ti9"}},{"cell_type":"code","source":["# Final results per BitFit/LoRA/Prompt Tuning Implementation\n","\n","albert_bf_results = pd.read_csv('/sent_albert_bitfit_results.csv')\n","albert_lora_results = pd.read_csv('/sent_albert_lora_results.csv')\n","albert_prompt_results = pd.read_csv('/sent_albert_prompt_results.csv')\n","\n","# Table of comparisons\n","comparison = pd.DataFrame({\n","    \"Method\": [\"BitFit\", \"LoRA\", \"Prompt Tuning\"],\n","    \"Best Validation F1\": [\n","        albert_bf_results[\"f1\"].max(),\n","        albert_lora_results[\"f1\"].max(),\n","        albert_prompt_results[\"f1\"].max()\n","    ],\n","    \"Best Validation Accuracy\": [\n","        albert_bf_results[\"accuracy\"].max(),\n","        albert_lora_results[\"accuracy\"].max(),\n","        albert_prompt_results[\"accuracy\"].max()\n","    ],\n","    \"Runtime (sec)\": [\n","        albert_bf_results[\"training_time\"].sum(),\n","        albert_lora_results[\"training_time\"].sum(),\n","        albert_prompt_results[\"training_time\"].sum()\n","    ],\n","    \"Inference Time (sec)\": [\n","        albert_bf_results[\"inference_time\"].sum(),\n","        albert_lora_results[\"inference_time\"].sum(),\n","        albert_prompt_results[\"inference_time\"].sum()\n","    ],\n","    \"Max GPU Memory (GB)\": [\n","        albert_bf_results[\"max_memory\"].max(),\n","        albert_lora_results[\"max_memory\"].max(),\n","        albert_prompt_results[\"max_memory\"].max()\n","    ]\n","})\n","\n","print(\"\\nFinal Validation Performance PEFT Comparison - ALBERT on Sentiment140:\")\n","display(comparison)\n"],"metadata":{"id":"9cyyi5LRfHkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load best inference metric summaries\n","bf_best_lr_tag\n","\n","bf_inf = pd.read_csv(f'/sent_albert_bitfit_inference_metrics_summary_lr{bf_best_lr_tag}_bs{bf_best_bs}.csv')\n","lora_inf = pd.read_csv(f'/sent_albert_lora_inference_metrics_summary_lr{lora_best_lr_tag}_bs{lora_best_bs}.csv')\n","prompt_inf = pd.read_csv(f'/sent_albert_prompt_inference_metrics_summary_lr{prompt_best_lr_tage}_bs{prompt_best_bs}.csv')\n","\n","# Table of best per-implementation metrics (based on best lr and bs per PEFT method)\n","final_test_results = pd.DataFrame({\n","    \"Method\": [\"BitFit\", \"LoRA\", \"Prompt Tuning\"],\n","    \"Test Accuracy\": [\n","        bf_inf.loc[0, \"accuracy\"],\n","        lora_inf.loc[0, \"accuracy\"],\n","        prompt_inf.loc[0, \"accuracy\"]\n","    ],\n","    \"F1 Macro\": [\n","        bf_inf.loc[0, \"f1_macro\"],\n","        lora_inf.loc[0, \"f1_macro\"],\n","        prompt_inf.loc[0, \"f1_macro\"]\n","    ],\n","    \"F1 Weighted\": [\n","        bf_inf.loc[0, \"f1_weighted\"],\n","        lora_inf.loc[0, \"f1_weighted\"],\n","        prompt_inf.loc[0, \"f1_weighted\"]\n","    ],\n","    \"Inference Time (sec)\": [\n","        bf_inf.loc[0, \"inference_time_sec\"],\n","        lora_inf.loc[0, \"inference_time_sec\"],\n","        prompt_inf.loc[0, \"inference_time_sec\"]\n","    ]\n","})\n","\n","print(\"\\nFinal Test Set Inference Performance PEFT Comparison - ALBERT on Sentiment140:\")\n","display(final_test_results)"],"metadata":{"id":"lJyb4IamfIIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ALBERT Sentiment Analysis Experiments on IMDb 50k Dataset using baseline model (and with PEFT -- LoRA, BitFit, Prompt Tuning)"],"metadata":{"id":"JGw7b7DOLHyR"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"661832b4-52e2-42d1-be1d-fbe3338ba0e9","id":"h8psMGbeLHyS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["\"\"\"We begin our process by installing packages such as pytorch, which is used extensively here, as well as HuggingFace's\n","transformers and datasets packages, which are used to run the ALBERT transformer model and load the IMDb 50k dataset, respectively. \"\"\"\n","\n","!pip install torch transformers datasets -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE-BzSk8LHyT"},"outputs":[],"source":["\"\"\"This step configures the credentials of the active user to seemlessly enable push and pull to and from the group's X-PERTS github repository\"\"\"\n","\n","!git config --global credential.helper store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGRF0rAFLHyT"},"outputs":[],"source":["\"\"\"We next import the installed packages, namely the ALBERT model \"\"\"\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset, concatenate_datasets\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n","\n","import time\n","from sklearn.metrics import classification_report, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"0A3ViX-iLHyT","executionInfo":{"status":"error","timestamp":1745740419468,"user_tz":240,"elapsed":27,"user":{"displayName":"Jack Henderson","userId":"11950955252015448237"}},"outputId":"9661013c-f870-42bd-b636-ace8948277c7"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"closing parenthesis '}' does not match opening parenthesis '[' (<ipython-input-1-ff561ea81177>, line 10)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ff561ea81177>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    dataset = {\"test\": full_imdb_split[test}\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis '}' does not match opening parenthesis '['\n"]}],"source":["\"\"\" We next instantiate (load) our IMDb 50k dataset\"\"\"\n","\n","dataset_imdb = load_dataset(\"imdb\")\n","\n","full_imdb = concatenate_datasets([dataset_imdb[\"train\"], dataset_imdb[\"test\"]])\n","\n","full_imdb_split = full_imdb.train_test_split(test_size=0.2, seed=42)\n","\n","full_train = full_imdb_split[\"train\"]\n","dataset = {\"test\": full_imdb_split[\"test\"]}\n","\n","print(\"Train size:\", len(full_train))\n","print(\"Test size:\", len(dataset[\"test\"]))"]},{"cell_type":"code","source":["def tokenize(example):\n","    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","tokenized_train = full_train.map(tokenize, batched=True)\n","tokenized_test = dataset[\"test\"].map(tokenize, batched=True)\n","\n","tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n","tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n","\n","tokenized_dataset = {\"train\": tokenized_train, \"test\": tokenized_test}"],"metadata":{"id":"79ljvhFoTTtj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300,"referenced_widgets":["6237ee14eb76420ea4f0b00ec2c429d7","eb3c9c5779ef40089ef03fa31be391af","bdb937fc686c4b6a8718acf1ffe79f34","32dc049651384edd861700c3688222a7","d4a01c1d1d6b4abcbe1cffc15b6f77da","0f5a317c92bd44b3a50239cc2003add9","1306c9c2070c49859516ed6e81664a45","14daacb74e764e7fa8837aad37fa6e85","044a502e62c5468c9378207a07088079","1c29c1b029d941138f6cf5a6b309975d","88eec9ad20744565b44899c42325e242","98fee8644dc34b63b0e971f4e15b73dc","88eb510e1fa94e6bb7ac10b82cfdb10c","741f23b7a1264b9f90a98825808b2e14","afe3260013174b2faa3bee6d85a5038b","c8857d36ff41427494a62f3fc9fd7851","12c8235eacce471c8d87108e2d282cd9","14589d46fa1142aead6095b6ef977f4a","12e04fedeac342a3aa1868484e20be50","64ea6e844cea42e8a8c4acd2c66d357c","60bace568fdb4ee885da2af5ef0a2811","248723bb73274730b42acf377e5db0bc","73b2b3e9a0194ffc91fa17174a3a56a5","fd9e55de3c6c4f90995ae54256da74e3","134d37b253ba4cdbbd65504eafacffea","11bacb8a9f4e414fb02e5ce36618ed85","9257e5bc2f2b4cdab4a34ff4bf93e816","3da388a19ce34bc5a5193251c59d9f77","f57e909c63fa4c71b82a2762a22257ab","0ac56303727a4d05a57ca2554002f357","68f1af78221e458e88a8a6d89e80c87b","710b81cbb4124262bafcd634d4fcc776","67f080ba67874917886e72dbc6a23d9a","654af0f2611b495d9fedc6fb31a6e42c","3d5727aa91ed42b280dbb38a901436f6","d73b57405b644957bf3cf2110f5578f9","5176926467ae4e8aa2cc746bea95d7df","a1fbd83f0f224f51ad725de3bf72c292","e92f5baefc474071b2f0c57d8047de4d","67151232362849708a24cc3f0872efa5","28f3005caa944c7c870427b3a502788c","ebdd79e2a91b4db6a7aff938a5f427e1","cc29f085002a4ccd963c78db05192fca","006cd7e16d554cf19df74d46e6f9af38","db115df5eb734d9586c6e65b0e249848","fe165e0a411b45bc815dc8cb8bf94152","a835e17fb3454feaa3e4df3c7aa2329b","bfb4910f78924b63bccf8abc7cb3f2c0","466a32afac2847b3b516748b9e7a4173","bc7f63c9e8f4441a85f697a646979bb1","27e847b1304748a2bdd0577eb354f4d0","7cdb15e7c3e94928bb9d9ec2bef1e096","99754e54baa0460486fde7827e071487","28f564aee43e4c9f97b9ac2acf434353","4868e5022b2b48e2bb9bab29bb829b45","19ebc8bbb912450bb0e3a830dc0e7a8a","f7c8c52e73bd4eabb760c4beb2fe821e","eaa821e6b39942159cb3860b830968e5","8c39643929b246cabdd6bb8e4d60d1f7","3d2edb44040c4e37bf8dca05b5d1c484","c9bacbb9bba441069588d872341046ec","43300c29859f4bde8c08b3ed1ef3523f","3e6c6355f41f4a85af51dccf6e198c39","c2223ecfc5d74523a36a20ed3b1f3d54","f590c23799204d80a2599b2e7b059ba5","a741e086a94f489b9b190f7e494bf644"]},"outputId":"fd825a32-5eb9-4940-cde2-0bc80296b6a7","id":"-qJ0zgAdLHyT"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6237ee14eb76420ea4f0b00ec2c429d7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98fee8644dc34b63b0e971f4e15b73dc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73b2b3e9a0194ffc91fa17174a3a56a5","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"654af0f2611b495d9fedc6fb31a6e42c","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db115df5eb734d9586c6e65b0e249848","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19ebc8bbb912450bb0e3a830dc0e7a8a","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_name = \"albert-base-v2\"\n","num_labels = 2\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"]},{"cell_type":"code","source":["\"\"\" We print the head of each of the train/test sets to visualize our cleaned data\"\"\"\n","\n","print(\"\\nSample training examples:\")\n","display(full_train[:5])\n","\n","print(\"\\nSample test examples:\")\n","display(dataset[\"test\"][:5])"],"metadata":{"id":"D6UtsMYNLHyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zc6iUKPVLHyU"},"outputs":[],"source":["\"\"\" We initialize our dataloader for each of the sets, fix their batch sizes\n","and randomize their order\"\"\"\n","\n","train_loader = DataLoader(tokenized_dataset[\"train\"], batch_size=16, shuffle=True)\n","test_loader  = DataLoader(tokenized_dataset[\"test\"], batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"outputId":"890c90ab-8b1b-4a35-e78d-795dd7e85e18","id":"4-OZiNXZLHyU"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test Accuracy   : 0.4466\n","F1 Score (macro): 0.4008\n","F1 Score (weighted): 0.4008\n","Inference Time  : 139.21s\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.38      0.17      0.24     12000\n","    Positive       0.47      0.72      0.57     12000\n","\n","    accuracy                           0.45     24000\n","   macro avg       0.42      0.45      0.40     24000\n","weighted avg       0.42      0.45      0.40     24000\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 600x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\" Baseline inference for binary sentiment analysis task run on ALBERT\n","without PEFT (i.e. without BitFit and/or LoRA)\"\"\"\n","\n","import time\n","import torch\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","inference_start = time.time()\n","\n","model.eval()\n","total_correct = 0\n","total_samples = 0\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","\n","        all_preds.extend(predictions.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","        total_correct += (predictions == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","accuracy = total_correct / total_samples\n","f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n","f1_weighted = f1_score(all_labels, all_preds, average=\"weighted\")\n","inference_time = time.time() - inference_start\n","\n","print(f'\\nBaseline Inference Performance - ALBERT on IMDb50k\\n')\n","print(f\"\\nTest Accuracy   : {accuracy:.4f}\")\n","print(f\"F1 Score (macro): {f1_macro:.4f}\")\n","print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","print(f\"Inference Time  : {inference_time:.2f}s\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"]))\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - Baseline Inference (ALBERT on IMDb50k)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Oi2CtJz3LHyU"},"source":["# LORA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"93e8b109-915e-4ee4-bfb3-e5320c7b10c4","id":"YheH8bXqLHyU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n","Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"]}],"source":["\"\"\" Install Parameter Efficient Finetuning Packages (e.g. LoRA and BitFit)\"\"\"\n","\n","!pip install peft -q"]},{"cell_type":"code","source":["\"\"\" Importing LoRA packages \"\"\"\n","\n","import gc\n","import torch\n","import time\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n","from peft import get_peft_model, LoraConfig, TaskType\n","from sklearn.metrics import classification_report, f1_score\n","from torch.utils.data import DataLoader"],"metadata":{"id":"eqOEucfHLHyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" LoRA parameter setup \"\"\"\n","\n","learning_rates = [5e-5, 1e-4]\n","batch_sizes = [8, 16]\n","epochs = 6"],"metadata":{"id":"XnCPresILHyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Training on ALBERT model using LoRA and output dataset generation (saved as .csv)\"\"\"\n","\n","results = []\n","\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","        print(f\"Running LoRA with LR={lr}, batch_size={batch_size}\")\n","\n","        # loading ALBERT model\n","        model_name = \"albert-base-v2\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","        # LoRA param update config\n","        lora_config = LoraConfig(\n","            task_type=TaskType.SEQ_CLS,\n","            r=16,\n","            lora_alpha=32,\n","            lora_dropout=0.1,\n","            bias=\"none\",\n","            target_modules=[\"query\", \"key\", \"value\"]\n","        )\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","\n","        # instantiate dataloader\n","        data_collator = DataCollatorWithPadding(tokenizer)\n","        train_dataloader = DataLoader(full_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n","        test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n","\n","        # adam optimizer\n","        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n","\n","        # begin training\n","        model.train()\n","        start_time = time.time()\n","        epoch_logs = []\n","\n","        for epoch in range(1, epochs + 1):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            loop = tqdm(train_dataloader, leave=False)\n","            for step, batch in enumerate(loop):\n","                if step >= 300:\n","                    break\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                correct += (preds == batch['labels']).sum().item()\n","                total += batch['labels'].size(0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            avg_train_loss = running_loss / (step + 1)\n","            train_accuracy = correct / total\n","\n","            # perform per epoch evaluation\n","            model.eval()\n","            val_running_loss = 0.0\n","            y_true, y_pred = [], []\n","            inference_start = time.time()\n","            with torch.no_grad():\n","                for batch in test_dataloader:\n","                    batch = {k: v.to(device) for k, v in batch.items()}\n","                    outputs = model(**batch)\n","                    preds = torch.argmax(outputs.logits, dim=1)\n","                    y_true.extend(batch[\"labels\"].cpu().numpy())\n","                    y_pred.extend(preds.cpu().numpy())\n","                    val_running_loss += outputs.loss.item()\n","\n","            avg_val_loss = val_running_loss / len(test_dataloader)\n","            inference_time = time.time() - inference_start\n","\n","            report = classification_report(y_true, y_pred, output_dict=True)\n","            val_accuracy = report[\"accuracy\"]\n","            val_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","            epoch_logs.append({\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": avg_train_loss,\n","                \"train_accuracy\": train_accuracy,\n","                \"val_loss\": avg_val_loss,\n","                \"val_accuracy\": val_accuracy\n","            })\n","\n","            if epoch == epochs:\n","                total_correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n","                total_samples = len(y_true)\n","                accuracy = total_correct / total_samples\n","                f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n","                f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n","\n","                print(f\"\\n[Final Epoch {epoch}] Inference Metrics:\")\n","                print(f\"Test Accuracy      : {accuracy:.4f}\")\n","                print(f\"F1 Score (macro)   : {f1_macro:.4f}\")\n","                print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","                print(f\"Inference Time     : {inference_time:.2f} seconds\")\n","                print(\"\\nClassification Report: ALBERT w/ LoRA on IMDb50k\\n\")\n","                print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n","\n","            model.train()\n","\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # begin datalogging per lr/bs\n","        epoch_logs_df = pd.DataFrame(epoch_logs)\n","        epoch_logs_df.to_csv(f\"imdb_albert_lora_epoch_logs_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # saver inference metrics per lr/bs\n","        metrics_summary_df = pd.DataFrame(report).transpose()\n","        metrics_summary_df.to_csv(f\"imdb_albert_lora_inference_metrics_summary_lr{lr}_bs{batch_size}.csv\", index=True)\n","\n","        # save inference predictions for the final epoch\n","        predictions_df = pd.DataFrame({\n","            \"y_true\": y_true,\n","            \"y_pred\": y_pred\n","        })\n","        predictions_df.to_csv(f\"imdb_albert_lora_inference_predictions_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # log memory usage\n","        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n","\n","        # save model params and metrics\n","        results.append({\n","            \"method\": \"LoRA\",\n","            \"learning_rate\": lr,\n","            \"batch_size\": batch_size,\n","            \"accuracy\": val_accuracy,\n","            \"f1\": val_f1,\n","            \"training_time\": training_time,\n","            \"inference_time\": inference_time,\n","            \"max_memory\": max_memory\n","        })\n","\n","        # empty cache to conserve compute\n","        del model, tokenizer, optimizer\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","# ranked performance by val acc\n","results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n","\n","# save overall results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"imdb_albert_lora_results.csv\", index=False)\n","\n","# save best final config and metrics\n","final_summary_df = pd.DataFrame({\n","    \"Method\": [\"LoRA\"],\n","    \"Best LR\": [results[0][\"learning_rate\"]],\n","    \"Best Batch Size\": [results[0][\"batch_size\"]],\n","    \"Accuracy\": [results[0][\"accuracy\"]],\n","    \"F1 Score\": [results[0][\"f1\"]],\n","    \"Training Time (s)\": [results[0][\"training_time\"]],\n","    \"Inference Time (s)\": [results[0][\"inference_time\"]],\n","    \"Max GPU Memory (GB)\": [results[0][\"max_memory\"]]\n","})\n","final_summary_df.to_csv(\"imdb_albert_lora_final_comparison_lora.csv\", index=False)\n","\n","print(\"All LoRA Grid Search Results:\")\n","for r in results:\n","    print(r)\n","\n","print(\"\\nBest LoRA Configuration:\")\n","print(results[0])"],"metadata":{"id":"hPpUbggJLHyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_best_lr = results[0][\"learning_rate\"]\n","lora_best_bs = results[0][\"batch_size\"]\n","\n","# Construct filename\n","best_report_file = f\"imdb_albert_lora_inference_metrics_summary_lr{lora_best_lr}_bs{lora_best_bs}.csv\"\n","\n","# Load the saved best report\n","best_report_df = pd.read_csv(best_report_file)\n","print(\"\\nClassification Report for Best Configuration:\")\n","print(best_report_df)\n","\n","\n","best_preds_df = pd.read_csv(f\"imdb_albert_lora_inference_predictions_lr{lora_best_lr}_bs{lora_best_bs}.csv\")\n","print(\"\\nInference Predictions for Best Configuration:\")\n","print(best_preds_df)\n","\n","y_true = best_preds_df[\"y_true\"]\n","y_pred = best_preds_df[\"y_pred\"]\n","\n","\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - ALBERT w/ LoRA on IMDb50k\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"error","timestamp":1745732643337,"user_tz":240,"elapsed":175,"user":{"displayName":"Jack Henderson","userId":"11950955252015448237"}},"outputId":"6449a0bb-6a70-441f-df7e-d283533db5a6","id":"fTudHvgXLHyV"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'results' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-49c4272d92e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Construct filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_report_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"opt_lora_inference_metrics_summary_lr{best_lr}_bs{best_bs}.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"BtUucFQ7LHyV"},"source":["# BITFIT"]},{"cell_type":"code","source":["\"\"\" Importing BitFit packages \"\"\"\n","\n","import gc\n","import torch\n","import time\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n","from peft import get_peft_model, LoraConfig, TaskType\n","from sklearn.metrics import classification_report, f1_score\n","from torch.utils.data import DataLoader"],"metadata":{"id":"0RkgMQ_9LHyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" BitFit parameter setup \"\"\"\n","learning_rates = [5e-5, 1e-4]\n","batch_sizes = [8, 16]\n","epochs = 6"],"metadata":{"id":"tMF03u7NLHyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Training on ALBERT model using BitFit and output dataset generation (saved as .csv)\"\"\"\n","\n","results = []\n","\n","for lr in learning_rates:\n","    for batch_size in batch_sizes:\n","        print(f\"Running BitFit with LR={lr}, batch_size={batch_size}\")\n","\n","        # loading ALBERT model\n","        model_name = \"albert-base-v2\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","        # BitFit param update config\n","        for name, param in model.named_parameters():\n","            if \"bias\" in name:\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","\n","        # instantiate dataloader\n","        data_collator = DataCollatorWithPadding(tokenizer)\n","        train_dataloader = DataLoader(full_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n","        test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n","\n","        # adam optimizer\n","        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n","\n","        # begin training\n","        model.train()\n","        start_time = time.time()\n","        epoch_logs = []\n","\n","        for epoch in range(1, epochs + 1):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            loop = tqdm(train_dataloader, leave=False)\n","            for step, batch in enumerate(loop):\n","                if step >= 300:\n","                    break\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                correct += (preds == batch['labels']).sum().item()\n","                total += batch['labels'].size(0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            avg_train_loss = running_loss / (step + 1)\n","            train_accuracy = correct / total\n","\n","            # perform per epoch evaluation\n","            model.eval()\n","            val_running_loss = 0.0\n","            y_true, y_pred = [], []\n","            inference_start = time.time()\n","            with torch.no_grad():\n","                for batch in test_dataloader:\n","                    batch = {k: v.to(device) for k, v in batch.items()}\n","                    outputs = model(**batch)\n","                    preds = torch.argmax(outputs.logits, dim=1)\n","                    y_true.extend(batch[\"labels\"].cpu().numpy())\n","                    y_pred.extend(preds.cpu().numpy())\n","                    val_running_loss += outputs.loss.item()\n","\n","            avg_val_loss = val_running_loss / len(test_dataloader)\n","\n","            inference_time = time.time() - inference_start\n","\n","            report = classification_report(y_true, y_pred, output_dict=True)\n","            val_accuracy = report[\"accuracy\"]\n","            val_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","            epoch_logs.append({\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": avg_train_loss,\n","                \"train_accuracy\": train_accuracy,\n","                \"val_loss\": avg_val_loss,\n","                \"val_accuracy\": val_accuracy\n","            })\n","\n","            if epoch == epochs:\n","                total_correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n","                total_samples = len(y_true)\n","                accuracy = total_correct / total_samples\n","                f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n","                f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n","\n","                print(f\"\\n[Final Epoch {epoch}] Inference Metrics:\")\n","                print(f\"Test Accuracy      : {accuracy:.4f}\")\n","                print(f\"F1 Score (macro)   : {f1_macro:.4f}\")\n","                print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","                print(f\"Inference Time     : {inference_time:.2f} seconds\")\n","                print(\"\\nClassification Report: ALBERT w/ BitFit on IMDb50k\\n\")\n","                print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n","\n","            model.train()\n","\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # begin datalogging per lr/bs\n","        epoch_logs_df = pd.DataFrame(epoch_logs)\n","        epoch_logs_df.to_csv(f\"imdb_albert_bitfit_epoch_logs_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # saver inference metrics per lr/bs\n","        metrics_summary_df = pd.DataFrame(report).transpose()\n","        metrics_summary_df.to_csv(f\"imdb_albert_bitfit_inference_metrics_summary_lr{lr}_bs{batch_size}.csv\", index=True)\n","\n","        # save inference predictions for the final epoch\n","        predictions_df = pd.DataFrame({\n","            \"y_true\": y_true,\n","            \"y_pred\": y_pred\n","        })\n","        predictions_df.to_csv(f\"imdb_albert_bitfit_inference_predictions_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # log memory usage\n","        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n","\n","        # save model params and metrics\n","        results.append({\n","            \"method\": \"BitFit\",\n","            \"learning_rate\": lr,\n","            \"batch_size\": batch_size,\n","            \"accuracy\": val_accuracy,\n","            \"f1\": val_f1,\n","            \"training_time\": training_time,\n","            \"inference_time\": inference_time,\n","            \"max_memory\": max_memory\n","        })\n","\n","        # empty cache to conserve compute\n","        del model, tokenizer, optimizer\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","# ranked performance by val acc\n","results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n","\n","# save overall results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"imdb_albert_bitfit_results.csv\", index=False)\n","\n","# save best final config and metrics\n","final_summary_df = pd.DataFrame({\n","    \"Method\": [\"BitFit\"],\n","    \"Best LR\": [results[0][\"learning_rate\"]],\n","    \"Best Batch Size\": [results[0][\"batch_size\"]],\n","    \"Accuracy\": [results[0][\"accuracy\"]],\n","    \"F1 Score\": [results[0][\"f1\"]],\n","    \"Training Time (s)\": [results[0][\"training_time\"]],\n","    \"Inference Time (s)\": [results[0][\"inference_time\"]],\n","    \"Max GPU Memory (GB)\": [results[0][\"max_memory\"]]\n","})\n","final_summary_df.to_csv(\"imdb_albert_bf_final_comparison_bitfit.csv\", index=False)\n","\n","print(\"All BitFit Grid Search Results:\")\n","for r in results:\n","    print(r)\n","\n","print(\"\\nBest BitFit Configuration:\")\n","print(results[0])\n"],"metadata":{"id":"yW-Cba1fLHyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bf_best_lr = results[0][\"learning_rate\"]\n","bf_best_bs = results[0][\"batch_size\"]\n","\n","# Construct filename\n","best_report_file = f\"imdb_albert_bitfit_inference_metrics_summary_lr{bf_best_lr}_bs{bf_best_bs}.csv\"\n","\n","# Load the saved best report\n","best_report_df = pd.read_csv(best_report_file)\n","print(\"\\nClassification Report for Best Configuration:\")\n","print(best_report_df)\n","\n","\n","best_preds_df = pd.read_csv(f\"imdb_albert_bitfit_inference_predictions_lr{bf_best_lr}_bs{bf_best_bs}.csv\")\n","print(\"\\nInference Predictions for Best Configuration:\")\n","print(best_preds_df)\n","\n","y_true = best_preds_df[\"y_true\"]\n","y_pred = best_preds_df[\"y_pred\"]\n","\n","\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - ALBERT w/ BitFit on IMDb50k\")\n","plt.show()"],"metadata":{"id":"KLIYd7jSLHyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prompt Tuning"],"metadata":{"id":"i08fJachLHyW"}},{"cell_type":"code","source":["\"\"\" Importing prompt tuning packages from PEFT \"\"\"\n","\n","import gc\n","from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType"],"metadata":{"id":"fU5erhTOLHyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Prompt tuning parameter setup \"\"\"\n","\n","lrs = [5e-5, 1e-4]\n","bs = [8, 16]\n","num_tokens = 20\n","epochs = 6"],"metadata":{"id":"ASvbGBIcLHyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Training and evaluation loop with hyperparamter grid search \"\"\"\n","\n","results = []\n","\n","for lr in lrs:\n","    for batch_size in bs:\n","        print(f\"Running Prompt Tuning with LR={lr}, batch_size={batch_size}\")\n","\n","        # loading ALBERT model\n","        model_name = \"albert-base-v2\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","        # prompt tuning config\n","        peft_config = PromptTuningConfig(\n","            task_type=TaskType.SEQ_CLS,\n","            num_virtual_tokens=num_tokens,\n","            tokenizer_name_or_path=tokenizer.name_or_path,\n","            prompt_tuning_init=PromptTuningInit.RANDOM,\n","        )\n","        prompt_model = get_peft_model(model, peft_config)\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        prompt_model.to(device)\n","\n","        # instantiate dataloader\n","        data_collator = DataCollatorWithPadding(tokenizer)\n","        train_dataloader = DataLoader(full_train, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n","        test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n","\n","        # adam optimization\n","        optimizer = torch.optim.AdamW(prompt_model.parameters(), lr=lr)\n","\n","        # begin training\n","        prompt_model.train()\n","        start_time = time.time()\n","        epoch_logs = []\n","\n","        for epoch in range(1, epochs + 1):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            loop = tqdm(train_dataloader, leave=False)\n","            for step, batch in enumerate(loop):\n","                if step >= 300:\n","                    break\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = prompt_model(**batch)\n","                loss = outputs.loss\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                correct += (preds == batch['labels']).sum().item()\n","                total += batch['labels'].size(0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            avg_train_loss = running_loss / (step + 1)\n","            train_accuracy = correct / total\n","\n","            # perform per epoch evaluation\n","            prompt_model.eval()\n","            val_running_loss = 0.0\n","            y_true, y_pred = [], []\n","            with torch.no_grad():\n","                for batch in test_dataloader:\n","                    batch = {k: v.to(device) for k, v in batch.items()}\n","                    outputs = prompt_model(**batch)\n","                    preds = torch.argmax(outputs.logits, dim=1)\n","                    y_true.extend(batch[\"labels\"].cpu().numpy())\n","                    y_pred.extend(preds.cpu().numpy())\n","                    val_running_loss += outputs.loss.item()\n","\n","            avg_val_loss = val_running_loss / len(test_dataloader)\n","\n","            inference_time = time.time() - start_time\n","\n","            report = classification_report(y_true, y_pred, output_dict=True)\n","            val_accuracy = report[\"accuracy\"]\n","            val_f1 = report[\"weighted avg\"][\"f1-score\"]\n","\n","            # print classification report on final epoch\n","            if epoch == epochs:\n","                total_correct = np.sum(np.array(y_true) == np.array(y_pred))\n","                total_samples = len(y_true)\n","\n","                accuracy = total_correct / total_samples\n","                f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n","                f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n","\n","                print(f\"\\n[Final Epoch {epoch}] Inference Metrics:\")\n","                print(f\"Test Accuracy      : {accuracy:.4f}\")\n","                print(f\"F1 Score (macro)   : {f1_macro:.4f}\")\n","                print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n","                print(f\"Inference Time     : {inference_time:.2f} seconds\")\n","                print(\"\\nClassification Report: ALBERT w/ Prompt Tuning on IMDb50k\\n\")\n","                print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n","\n","            epoch_logs.append({\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": avg_train_loss,\n","                \"train_accuracy\": train_accuracy,\n","                \"val_loss\": avg_val_loss,\n","                \"val_accuracy\": val_accuracy\n","            })\n","\n","            prompt_model.train()\n","\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # begin datalogging per lr/bs\n","        epoch_logs_df = pd.DataFrame(epoch_logs)\n","        epoch_logs_df.to_csv(f\"imdb_albert_prompt_epoch_logs_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # save inference metrics per lr/bs\n","        metrics_summary_df = pd.DataFrame(report).transpose()\n","        metrics_summary_df.to_csv(f\"imdb_albert_prompt_inference_metrics_summary_lr{lr}_bs{batch_size}.csv\", index=True)\n","\n","        # Save inference predictions for the final epoch\n","        predictions_df = pd.DataFrame({\n","            \"y_true\": y_true,\n","            \"y_pred\": y_pred\n","        })\n","        predictions_df.to_csv(f\"imdb_albert_prompt_inference_predictions_lr{lr}_bs{batch_size}.csv\", index=False)\n","\n","        # log memory usage\n","        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0\n","\n","        # save model params and metrics\n","        results.append({\n","            \"method\": \"Prompt Tuning\",\n","            \"learning_rate\": lr,\n","            \"batch_size\": batch_size,\n","            \"accuracy\": val_accuracy,\n","            \"f1\": val_f1,\n","            \"training_time\": training_time,\n","            \"max_memory\": max_memory\n","        })\n","\n","        # empty cache to conserve compute\n","        del prompt_model, model, tokenizer, optimizer\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","# ranked performance by val acc\n","results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n","\n","# save overall results\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"imdb_albert_prompt_results.csv\", index=False)\n","\n","# save best final config and metrics\n","final_summary_df = pd.DataFrame({\n","    \"Method\": [\"Prompt Tuning\"],\n","    \"Best LR\": [results[0][\"learning_rate\"]],\n","    \"Best Batch Size\": [results[0][\"batch_size\"]],\n","    \"Accuracy\": [results[0][\"accuracy\"]],\n","    \"F1 Score\": [results[0][\"f1\"]],\n","    \"Training Time (s)\": [results[0][\"training_time\"]],\n","    \"Max GPU Memory (GB)\": [results[0][\"max_memory\"]]\n","})\n","final_summary_df.to_csv(\"imdb_albert_prompt_final_comparison_prompt_tuning.csv\", index=False)\n","\n","print(\"All Prompt Tuning Grid Search Results:\")\n","for r in results:\n","    print(r)\n","\n","print(\"\\nBest Configuration:\")\n","print(results[0])"],"metadata":{"id":"NCJRBSEWLHyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_best_lr = results[0][\"learning_rate\"]\n","prompt_best_bs = results[0][\"batch_size\"]\n","\n","# Construct filename\n","best_report_file = f\"imdb_albert_prompt_inference_metrics_summary_lr{prompt_best_lr}_bs{prompt_best_bs}.csv\"\n","\n","# Load the saved best report\n","best_report_df = pd.read_csv(best_report_file)\n","print(\"\\nClassification Report for Best Configuration:\")\n","print(best_report_df)\n","\n","\n","best_preds_df = pd.read_csv(f\"imdb_albert_prompt_inference_predictions_lr{prompt_best_lr}_bs{prompt_best_bs}.csv\")\n","print(\"\\nInference Predictions for Best Configuration:\")\n","print(best_preds_df)\n","\n","y_true = best_preds_df[\"y_true\"]\n","y_pred = best_preds_df[\"y_pred\"]\n","\n","\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix - ALBERT w/ Prompt Tuning on IMDb50k\")\n","plt.show()"],"metadata":{"id":"h-yOoyI7LHyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Begin Visualization of IMDb 50k Results"],"metadata":{"id":"-KQTQxOwLHyW"}},{"cell_type":"markdown","source":["Load all dataframes from above training of 3 PEFT methods"],"metadata":{"id":"yOY_AQvsLHyW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"17FZm-DLLHyW"},"outputs":[],"source":["\"\"\" Output from our ALBERT_Sentiment140_IMDb50k.ipynb file, we read in the .csv files here.\n","Note that these .csv file paths suggest they should be uploaded to session storage\"\"\"\n","\n","# ALBERT PEFT-wise results across bf/lora/prompt tuning\n","albert_bf_results = pd.read_csv('/imdb_albert_bitfit_results.csv')\n","albert_lora_results = pd.read_csv('/imdb_albert_lora_results.csv')\n","albert_prompt_results = pd.read_csv('/imdb_albert_prompt_results.csv')\n","\n","# ALBERT per-epoch performance logs (for LC generation) across bf/lora/prompt tuning\n","albert_lora_epochs_lr5_bs8 = pd.read_csv('/imdb_albert_lora_epoch_logs_lr5e-05_bs8.csv')\n","albert_lora_epochs_lr5_bs16 = pd.read_csv('/imdb_albert_lora_epoch_logs_lr5e-05_bs16.csv')\n","albert_lora_epochs_lr1_bs8 = pd.read_csv('/imdb_albert_lora_epoch_logs_lr1e-04_bs8.csv')\n","albert_lora_epochs_lr1_bs16 = pd.read_csv('/imdb_albert_lora_epoch_logs_lr1e-04_bs16.csv')\n","albert_bf_epochs_lr5_bs8 = pd.read_csv('/imdb_albert_bitfit_epoch_logs_lr5e-05_bs8.csv')\n","albert_bf_epochs_lr5_bs16 = pd.read_csv('/imdb_albert_bitfit_epoch_logs_lr5e-05_bs16.csv')\n","albert_bf_epochs_lr1_bs8 = pd.read_csv('/imdb_albert_bitfit_epoch_logs_lr1e-04_bs8.csv')\n","albert_bf_epochs_lr1_bs16 = pd.read_csv('/imdb_albert_bitfit_epoch_logs_lr1e-04_bs16.csv')\n","albert_prompt_epochs_lr5_bs8 = pd.read_csv('/imdb_albert_prompt_epoch_logs_lr5e-05_bs8.csv')\n","albert_prompt_epochs_lr5_bs16 = pd.read_csv('/imdb_albert_prompt_epoch_logs_lr5e-05_bs16.csv')\n","albert_prompt_epochs_lr1_bs8 = pd.read_csv('/imdb_albert_prompt_epoch_logs_lr1e-04_bs8.csv')\n","albert_prompt_epochs_lr1_bs16 = pd.read_csv('/imdb_albert_prompt_epoch_logs_lr1e-04_bs16.csv')\n","\n","# ALBERT inference performance metric summary across bf/lora/prompt tuning\n","albert_bf_inf_lr5_bs8 = pd.read_csv('/imdb_albert_bitfit_inference_metrics_summary_lr5e-05_bs8.csv')\n","albert_bf_inf_lr5_bs16 = pd.read_csv('/imdb_albert_bitfit_inference_metrics_summary_lr5e-05_bs16.csv')\n","albert_bf_inf_lr1_bs8 = pd.read_csv('/imdb_albert_bitfit_inference_metrics_summary_lr1e-04_bs8.csv')\n","albert_bf_inf_lr1_bs16 = pd.read_csv('/imdb_albert_bitfit_inference_metrics_summary_lr1e-04_bs16.csv')\n","albert_lora_inf_lr5_bs8 = pd.read_csv('/imdb_albert_lora_inference_metrics_summary_lr5e-05_bs8.csv')\n","albert_lora_inf_lr5_bs16 = pd.read_csv('/imdb_albert_lora_inference_metrics_summary_lr5e-05_bs16.csv')\n","albert_lora_inf_lr1_bs8 = pd.read_csv('/imdb_albert_lora_inference_metrics_summary_lr1e-04_bs8.csv')\n","albert_lora_inf_lr1_bs16 = pd.read_csv('/imdb_albert_lora_inference_metrics_summary_lr1e-04_bs16.csv')\n","albert_prompt_inf_lr5_bs8 = pd.read_csv('/imdb_albert_prompt_inference_metrics_summary_lr5e-05_bs8.csv')\n","albert_prompt_inf_lr5_bs16 = pd.read_csv('/imdb_albert_prompt_inference_metrics_summary_lr5e-05_bs16.csv')\n","albert_prompt_inf_lr1_bs8 = pd.read_csv('/imdb_albert_prompt_inference_metrics_summary_lr1e-04_bs8.csv')\n","albert_prompt_inf_lr1_bs16 = pd.read_csv('/imdb_albert_prompt_inference_metrics_summary_lr1e-04_bs16.csv')\n","\n","# ALBERT inference predictions across bf/lora/prompt tuning\n","albert_bf_preds_lr5_bs8 = pd.read_csv('/imdb_albert_bitfit_inference_predictions_lr5e-05_bs8.csv')\n","albert_bf_preds_lr5_bs16 = pd.read_csv('/imdb_albert_bitfit_inference_predictions_lr5e-05_bs16.csv')\n","albert_bf_preds_lr1_bs8 = pd.read_csv('/imdb_albert_bitfit_inference_predictions_lr1e-04_bs8.csv')\n","albert_bf_preds_lr1_bs16 = pd.read_csv('/imdb_albert_bitfit_inference_predictions_lr1e-04_bs16.csv')\n","albert_lora_preds_lr5_bs8 = pd.read_csv('/imdb_albert_lora_inference_predictions_lr5e-05_bs8.csv')\n","albert_lora_preds_lr5_bs16 = pd.read_csv('/imdb_albert_lora_inference_predictions_lr5e-05_bs16.csv')\n","albert_lora_preds_lr1_bs8 = pd.read_csv('/imdb_albert_lora_inference_predictions_lr1e-04_bs8.csv')\n","albert_lora_preds_lr1_bs16 = pd.read_csv('/imdb_albert_lora_inference_predictions_lr1e-04_bs16.csv')\n","albert_prompt_preds_lr5_bs8 = pd.read_csv('/imdb_albert_prompt_inference_predictions_lr5e-05_bs8.csv')\n","albert_prompt_preds_lr5_bs16 = pd.read_csv('/imdb_albert_prompt_inference_predictions_lr5e-05_bs16.csv')\n","albert_prompt_preds_lr1_bs8 = pd.read_csv('/imdb_albert_prompt_inference_predictions_lr1e-04_bs8.csv')\n","albert_prompt_preds_lr1_bs16 = pd.read_csv('/imdb_albert_prompt_inference_predictions_lr1e-04_bs16.csv')\n","\n","# ALBERT PEFT method intra-comparison based on hyperparameter settings, per bf/lora/prompt tuning\n","albert_bf_final_comparison = pd.read_csv('/imdb_albert_bf_final_comparison_bitfit.csv')\n","albert_lora_final_comparison = pd.read_csv('/imdb_albert_lora_final_comparison_lora.csv')\n","albert_prompt_final_comparison = pd.read_csv('/imdb_albert_prompt_final_comparison_prompt_tuning.csv')\n"]},{"cell_type":"markdown","source":["BitFit Learning Curves"],"metadata":{"id":"BplbKs2bLHyX"}},{"cell_type":"code","source":["# All BitFit Train/Val Acc Learning Curve\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs16\")\n","plt.title(\"Learning Curve: Accuracy - ALBERT w/ BitFit on IMDb50k\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# All BitFit Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr5_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs8\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs16\")\n","sns.lineplot(data=albert_bf_epochs_lr1_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs16\")\n","plt.title(\"Learning Curve: Loss - ALBERT w/ BitFit on IMDb50k\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"5JR_NBfVLHyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best BitFit Train/Val Acc Learning Curve\n","\n","albert_bf_epochs_map = {\n","    (5, 8): albert_bf_epochs_lr5_bs8,\n","    (5, 16): albert_bf_epochs_lr5_bs16,\n","    (1, 8): albert_bf_epochs_lr1_bs8,\n","    (1, 16): albert_bf_epochs_lr1_bs16\n","}\n","\n","bf_lr_mapping = {\n","    5e-5: 5,\n","    1e-4: 1\n","}\n","\n","bf_best_lr_tag = bf_lr_mapping[bf_best_lr]\n","bf_best_bs_tag = bf_best_bs\n","\n","bf_epochs = albert_bf_epochs_map[(bf_best_lr_tag, bf_best_bs_tag)]\n","\n","# Best BitFit Training and Validation Accuracy\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"train_accuracy\", label=\"Training Accuracy\")\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"val_accuracy\", label=\"Validation Accuracy\")\n","plt.title(f\"Learning Curve: Best Accuracy - ALBERT w/ BitFit on IMDb50k\\nBest Hyperparameters: LR: {bf_best_lr} and BS: {bf_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Best BitFit Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"train_loss\", label=\"Training Loss\")\n","sns.lineplot(data=bf_epochs, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n","plt.title(f\"Learning Curve: Best Loss - ALBERT w/ BitFit on IMDb50k\\nBest Hyperparameters: LR: {bf_best_lr} and BS: {bf_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"EWoLLnT3LHyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LoRA Learning Curves"],"metadata":{"id":"gslIATIJLHyX"}},{"cell_type":"code","source":["# All LoRA Train/Val Acc Learning Curve (ALBERT)\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs16\")\n","plt.title(\"Learning Curve: Accuracy - ALBERT w/ LoRA on IMDb50k\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# All LoRA Training and Validation Loss (ALBERT)\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr5_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs8\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs16\")\n","sns.lineplot(data=albert_lora_epochs_lr1_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs16\")\n","plt.title(\"Learning Curve: Loss - ALBERT w/ LoRA on IMDb50k\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"KpOtoR3ILHyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best LoRA Train/Val Acc Learning Curve\n","\n","albert_lora_epochs_map = {\n","    (5, 8): albert_lora_epochs_lr5_bs8,\n","    (5, 16): albert_lora_epochs_lr5_bs16,\n","    (1, 8): albert_lora_epochs_lr1_bs8,\n","    (1, 16): albert_lora_epochs_lr1_bs16\n","}\n","\n","lora_lr_mapping = {\n","    5e-5: 5,\n","    1e-4: 1\n","}\n","\n","lora_best_lr_tag = lora_lr_mapping[lora_best_lr]\n","lora_best_bs_tag = lora_best_bs\n","\n","lora_epochs = albert_lora_epochs_map[(lora_best_lr_tag, lora_best_bs_tag)]\n","\n","# Best LoRA Training and Validation Accuracy\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"train_accuracy\", label=\"Training Accuracy\")\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"val_accuracy\", label=\"Validation Accuracy\")\n","plt.title(f\"Learning Curve: Best Accuracy - ALBERT w/ LoRA on IMDb50k\\nBest Hyperparameters: LR: {lora_best_lr} and BS: {lora_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Best LoRA Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"train_loss\", label=\"Training Loss\")\n","sns.lineplot(data=lora_epochs, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n","plt.title(f\"Learning Curve: Best Loss - ALBERT w/ LoRA on IMDb50k\\nBest Hyperparameters: LR: {lora_best_lr} and BS: {lora_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"t4Ha1l8fLHyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prompt Tuning Learning Curves"],"metadata":{"id":"XdQ8hOrOLHyX"}},{"cell_type":"code","source":["# All Prompt Tuning Train/Val Acc Learning Curve\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"train_accuracy\", label=\"TA_lr1_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"val_accuracy\", label=\"VA_lr1_bs16\")\n","plt.title(\"Learning Curve: Accuracy - ALBERT w/ Prompt Tuning on IMDb50k\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# All Prompt Tuning Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr5_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr5_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs8, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs8\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"train_loss\", label=\"TL_lr1_bs16\")\n","sns.lineplot(data=albert_prompt_epochs_lr1_bs16, x=\"epoch\", y=\"val_loss\", label=\"VL_lr1_bs16\")\n","plt.title(\"Learning Curve: Loss - ALBERT w/ Prompt Tuning on IMDb50k\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"7EunxteKLHyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best Prompt Tuning Train/Val Acc Learning Curve\n","\n","albert_prompt_epochs_map = {\n","    (5, 8): albert_prompt_epochs_lr5_bs8,\n","    (5, 16): albert_prompt_epochs_lr5_bs16,\n","    (1, 8): albert_prompt_epochs_lr1_bs8,\n","    (1, 16): albert_prompt_epochs_lr1_bs16\n","}\n","\n","prompt_lr_mapping = {\n","    5e-5: 5,\n","    1e-4: 1\n","}\n","\n","prompt_best_lr_tag = prompt_lr_mapping[prompt_best_lr]\n","prompt_best_bs_tag = prompt_best_bs\n","\n","prompt_epochs = albert_prompt_epochs_map[(prompt_best_lr_tag, prompt_best_bs_tag)]\n","\n","# Best Prompt Tuning Training and Validation Accuracy\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"train_accuracy\", label=\"Training Accuracy\")\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"val_accuracy\", label=\"Validation Accuracy\")\n","plt.title(f\"Learning Curve: Best Accuracy - ALBERT w/ Prompt Tuning on IMDb50k\\nBest Hyperparameters: LR: {prompt_best_lr} and BS: {prompt_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Best Prompt Tuning Training and Validation Loss\n","plt.figure(figsize=(10,5))\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"train_loss\", label=\"Training Loss\")\n","sns.lineplot(data=prompt_epochs, x=\"epoch\", y=\"val_loss\", label=\"Validation Loss\")\n","plt.title(f\"Learning Curve: Best Loss - ALBERT w/ Prompt Tuning on IMDb50k\\nBest Hyperparameters: LR: {prompt_best_lr} and BS: {prompt_best_bs}\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"UTr3QE8sLHyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PEFT Method Comparison:"],"metadata":{"id":"5eHKvrGALHyX"}},{"cell_type":"code","source":["# Final results per BitFit/LoRA/Prompt Tuning Implementation\n","\n","albert_bf_results = pd.read_csv('/imdb_albert_bitfit_results.csv')\n","albert_lora_results = pd.read_csv('/imdb_albert_lora_results.csv')\n","albert_prompt_results = pd.read_csv('/imdb_albert_prompt_results.csv')\n","\n","# Table of comparisons\n","comparison = pd.DataFrame({\n","    \"Method\": [\"BitFit\", \"LoRA\", \"Prompt Tuning\"],\n","    \"Best Validation F1\": [\n","        albert_bf_results[\"f1\"].max(),\n","        albert_lora_results[\"f1\"].max(),\n","        albert_prompt_results[\"f1\"].max()\n","    ],\n","    \"Best Validation Accuracy\": [\n","        albert_bf_results[\"accuracy\"].max(),\n","        albert_lora_results[\"accuracy\"].max(),\n","        albert_prompt_results[\"accuracy\"].max()\n","    ],\n","    \"Runtime (sec)\": [\n","        albert_bf_results[\"training_time\"].sum(),\n","        albert_lora_results[\"training_time\"].sum(),\n","        albert_prompt_results[\"training_time\"].sum()\n","    ],\n","    \"Inference Time (sec)\": [\n","        albert_bf_results[\"inference_time\"].sum(),\n","        albert_lora_results[\"inference_time\"].sum(),\n","        albert_prompt_results[\"inference_time\"].sum()\n","    ],\n","    \"Max GPU Memory (GB)\": [\n","        albert_bf_results[\"max_memory\"].max(),\n","        albert_lora_results[\"max_memory\"].max(),\n","        albert_prompt_results[\"max_memory\"].max()\n","    ]\n","})\n","\n","print(\"\\nFinal Validation Performance PEFT Comparison - ALBERT on IMDb50k:\")\n","display(comparison)\n"],"metadata":{"id":"Y_43x8e9LHyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load best inference metric summaries\n","bf_best_lr_tag\n","\n","bf_inf = pd.read_csv(f'/imdb_albert_bitfit_inference_metrics_summary_lr{bf_best_lr_tag}_bs{bf_best_bs}.csv')\n","lora_inf = pd.read_csv(f'/imdb_albert_lora_inference_metrics_summary_lr{lora_best_lr_tag}_bs{lora_best_bs}.csv')\n","prompt_inf = pd.read_csv(f'/imdb_albert_prompt_inference_metrics_summary_lr{prompt_best_lr_tage}_bs{prompt_best_bs}.csv')\n","\n","# Table of best per-implementation metrics (based on best lr and bs per PEFT method)\n","final_test_results = pd.DataFrame({\n","    \"Method\": [\"BitFit\", \"LoRA\", \"Prompt Tuning\"],\n","    \"Test Accuracy\": [\n","        bf_inf.loc[0, \"accuracy\"],\n","        lora_inf.loc[0, \"accuracy\"],\n","        prompt_inf.loc[0, \"accuracy\"]\n","    ],\n","    \"F1 Macro\": [\n","        bf_inf.loc[0, \"f1_macro\"],\n","        lora_inf.loc[0, \"f1_macro\"],\n","        prompt_inf.loc[0, \"f1_macro\"]\n","    ],\n","    \"F1 Weighted\": [\n","        bf_inf.loc[0, \"f1_weighted\"],\n","        lora_inf.loc[0, \"f1_weighted\"],\n","        prompt_inf.loc[0, \"f1_weighted\"]\n","    ],\n","    \"Inference Time (sec)\": [\n","        bf_inf.loc[0, \"inference_time_sec\"],\n","        lora_inf.loc[0, \"inference_time_sec\"],\n","        prompt_inf.loc[0, \"inference_time_sec\"]\n","    ]\n","})\n","\n","print(\"\\nFinal Test Set Inference Performance PEFT Comparison - ALBERT on IMDb50k:\")\n","display(final_test_results)"],"metadata":{"id":"gy4N1__tLHyY"},"execution_count":null,"outputs":[]}]}